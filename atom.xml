<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Joseph Chee Chang]]></title>
  <link href="http://josephcc.github.com/atom.xml" rel="self"/>
  <link href="http://josephcc.github.com/"/>
  <updated>2022-10-28T14:43:00-07:00</updated>
  <id>http://josephcc.github.com/</id>
  <author>
    <name><![CDATA[Joseph Chee Chang]]></name>
    <email><![CDATA[josephc->allenai*org]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Threddy]]></title>
    <link href="http://josephcc.github.com/UIST-Threddy/"/>
    <updated>2022-10-29T07:00:00-07:00</updated>
    <id>http://josephcc.github.com/UIST-Threddy</id>
    <content type="html"><![CDATA[<p>Reviewing the literature to understand relevant past work is a critical part of
research. However, as the scientific literature grows the challenges for users
to find and make sense of the many different threads of research grow as well.
In this work we explore a tool integrated into users&#8217; reading process that
helps them with leveraging authors&#8217; existing summarization of prior research
threads such as in related work sections.  We developed Threddy that supports
efficient extraction and organization of threads along with supporting evidence
as scientists read research articles. The system then recommends further
relevant articles based on user-created threads.  Our lab study showed that
Threddy helps scientists to follow and curate research threads without breaking
out of their flow of reading, collect relevant papers and clips, and discover
interesting new articles to further grow threads.</p>

<!--more-->


<h2>Abstract</h2>

<p>Reviewing the literature to understand relevant threads of past work is a
critical part of research and vehicle for learning. However, as the scientific
literature grows the challenges for users to find and make sense of the many
different threads of research grow as well. Previous work has helped scholars
to find and group papers with citation information or textual similarity using
standalone tools or overview visualizations. Instead, in this work we explore a
tool integrated into users&#8217; reading process that helps them with leveraging
authors&#8217; existing summarization of threads, typically in introduction or
related work sections, in order to situate their own work&#8217;s contributions. To
explore this we developed a prototype that supports efficient extraction and
organization of threads along with supporting evidence as scientists read
research articles. The system then recommends further relevant articles based
on user-created threads. We evaluate the system in a lab study and find that it
helps scientists to follow and curate research threads without breaking out of
their flow of reading, collect relevant papers and clips, and discover
interesting new articles to further grow threads.</p>

<h2>Downloads</h2>

<p><a class="btn btn-default" href="http://josephcc.github.com/images/papers/threddy.pdf" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Threddy', 'PDF']);" role="button">PDF Download</a>
<a class="btn btn-default" href="https://arxiv.org/abs/2208.03455" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Threddy', 'arXiv']);"  role="button">arXiv</a></p>

<h2>Citation</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Hyeonsu B. Kang, Joseph Chee Chang, Yongsung Kim, Aniket Kittur
</span><span class='line'>"Threddy: An Interactive System for Personalized Thread-based Exploration and Organization of Scientific Literature"
</span><span class='line'>In Proceedings of the 35th ACM User Interface Software and Technology Symposium: UIST 2022.</span></code></pre></td></tr></table></div></figure>


<h2>Bibtex</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>@article{kang2022threddy,
</span><span class='line'> title={Threddy: An Interactive System for Personalized Thread-based Exploration and Organization of Scientific Literature},
</span><span class='line'> author={Kang, Hyeonsu B and Chang, Joseph Chee and Kim, Yongsung and Kittur, Aniket},
</span><span class='line'> booktitle = {},
</span><span class='line'> series = {UIST '22},
</span><span class='line'> year = {2022},
</span><span class='line'> location = {Bend, OR, USA},
</span><span class='line'> publisher = {ACM},
</span><span class='line'> address = {New York, NY, USA}
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Fuse]]></title>
    <link href="http://josephcc.github.com/UIST-Fuse/"/>
    <updated>2022-10-29T06:00:00-07:00</updated>
    <id>http://josephcc.github.com/UIST-Fuse</id>
    <content type="html"><![CDATA[<p>While people can fluidly collect, make sense of  and organize information
across many online sources in their minds to make informed decisions, the
amount of online information can be overwhelming and exceed people&#8217;s working
memory.  We introduce Fuse, a browser extension that combined low-cost
collection with lightweight organization of web content in a compact card-based
sidebar.  Fuse helps users simultaneously extract key web content and structure
it in a lightweight and visual way.  A 22-month public deployment and
interviews provide longitudinal insights into the collecting, externalization
and  structuring behaviors of real-world users conducting information foraging
tasks.</p>

<!--more-->


<h2>Abstract</h2>

<p>People spend a significant amount of time trying to make sense of the internet,
collecting content from a variety of sources and organizing it to make
decisions and achieve their goals. While humans are able to fluidly iterate on
collecting and organizing information in their minds, existing tools and
approaches introduce significant friction into the process. We introduce Fuse,
a browser extension that externalizes users&#8217; working memory by combining
low-cost collection with lightweight organization of content in a compact
card-based sidebar that is always available. Fuse helps users simultaneously
extract key web content and structure it in a lightweight and visual way. We
discuss how these affordances help users externalize more of their mental model
into the system (e.g., saving, annotating, and structuring items) and support
fast reviewing and resumption of task contexts. Our 22-month public deployment
and follow-up interviews provide longitudinal insights into the structuring
behaviors of real-world users conducting information foraging tasks.</p>

<h2>Downloads</h2>

<p><a class="btn btn-default" href="http://josephcc.github.com/images/papers/fuse.pdf" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Fuse', 'PDF']);" role="button">PDF Download</a>
<a class="btn btn-default" href="https://arxiv.org/abs/2208.14861" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Fuse', 'arXiv']);"  role="button">arXiv</a></p>

<h2>Citation</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Kuznetsov, Andrew, Joseph Chee Chang, Nathan Hahn, Napol Rachatasumrit, Bradley Breneisen, Julina Coupland, and Aniket Kittur.
</span><span class='line'>"Fuse: In-Situ Sensemaking Support in the Browser."
</span><span class='line'>In Proceedings of the 35th ACM User Interface Software and Technology Symposium: UIST 2022.</span></code></pre></td></tr></table></div></figure>


<h2>Bibtex</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>@article{kuznetsov2022fuse,
</span><span class='line'> title={Fuse: In-Situ Sensemaking Support in the Browser},
</span><span class='line'> author={Kuznetsov, Andrew and Chang, Joseph Chee and Hahn, Nathan and Rachatasumrit, Napol and Breneisen, Bradley and Coupland, Julina and Kittur, Aniket},
</span><span class='line'> booktitle = {},
</span><span class='line'> series = {UIST '22},
</span><span class='line'> year = {2022},
</span><span class='line'> location = {Bend, OR, USA},
</span><span class='line'> publisher = {ACM},
</span><span class='line'> address = {New York, NY, USA}
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Wigglite]]></title>
    <link href="http://josephcc.github.com/UIST-Wigglite/"/>
    <updated>2022-10-29T05:00:00-07:00</updated>
    <id>http://josephcc.github.com/UIST-Wigglite</id>
    <content type="html"><![CDATA[<p>Users conducting online sensemaking tasks face the challenge of capturing the
information they find for later use without interrupting their flow.
Specifically, as people collect information, they also need to triaging support
such as how urgent a topic is to follow up on, or rating a piece of evidence as
a &#8220;pro&#8221; or &#8220;con,&#8221; which helps scaffold subsequent deeper exploration.  However,
current approaches incur a high cost, often requiring users to select, copy,
context switch, paste, and annotate information.  In this work, we explore a
new interaction technique called &#8220;wiggling,&#8221; which can be used to fluidly
collect, organize, and rate information during early sensemaking stages with a
single gesture. Through implementation and user evaluation, we found that
wiggling helped participants accurately collect information and encode their
mental context with a 58% reduction in operational cost while being 24% faster
compared to a common baseline.</p>

<!--more-->


<h2>Abstract</h2>

<p>Consumers conducting comparison shopping, researchers making sense of
competitive space, and developers looking for code snippets online all face the
challenge of capturing the information they find for later use without
interrupting their current flow. In addition, during many learning and
exploration tasks, people need to externalize their mental context, such as
estimating how urgent a topic is to follow up on, or rating a piece of evidence
as a &#8220;pro&#8221; or &#8220;con,&#8221; which helps scaffold subsequent deeper exploration.
However, current approaches incur a high cost, often requiring users to select,
copy, context switch, paste, and annotate information in a separate document
without offering specific affordances that capture their mental context. In
this work, we explore a new interaction technique called &#8220;wiggling,&#8221; which can
be used to fluidly collect, organize, and rate information during early
sensemaking stages with a single gesture. Wiggling involves rapid
back-and-forth movements of a pointer or up-and-down scrolling on a smartphone,
which can indicate the information to be collected and its valence, using a
single, light-weight gesture that does not interfere with other interactions
that are already available. Through implementation and user evaluation, we
found that wiggling helped participants accurately collect information and
encode their mental context with a 58% reduction in operational cost while
being 24% faster compared to a common baseline.</p>

<h2>Downloads</h2>

<p><a class="btn btn-default" href="http://josephcc.github.com/images/papers/wigglite.pdf" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Wigglite', 'PDF']);" role="button">PDF Download</a>
<a class="btn btn-default" href="https://arxiv.org/abs/2208.00496" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Wigglite', 'arXiv']);"  role="button">arXiv</a></p>

<h2>Citation</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Michael Xieyang Liu, Andrew Kuznetsov, Yongsung Kim, Joseph Chee Chang, Aniket Kittur, Brad A Myers
</span><span class='line'>"Wigglite: Low-cost Information Collection and Triage"
</span><span class='line'>In Proceedings of the 35th ACM User Interface Software and Technology Symposium: UIST 2022.</span></code></pre></td></tr></table></div></figure>


<h2>Bibtex</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>@article{liu2022wigglite,
</span><span class='line'> title={Wigglite: Low-cost Information Collection and Triage},
</span><span class='line'> author={Liu, Michael Xieyang and Kuznetsov, Andrew and Kim, Yongsung and Chang, Joseph Chee and Kittur, Aniket and Myers, Brad A.},
</span><span class='line'> booktitle = {},
</span><span class='line'> series = {UIST '22},
</span><span class='line'> year = {2022},
</span><span class='line'> location = {Bend, OR, USA},
</span><span class='line'> publisher = {ACM},
</span><span class='line'> address = {New York, NY, USA}
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tabs.do]]></title>
    <link href="http://josephcc.github.com/UIST-tabsdo/"/>
    <updated>2021-10-10T08:00:00-07:00</updated>
    <id>http://josephcc.github.com/UIST-tabsdo</id>
    <content type="html"><![CDATA[<p>The nature of people’s online activities has gone through dramatic changes in
past decades, yet browser interfaces have stayed largely the same since tabs
were introduced nearly 20 years ago. The divide between browser interfaces that
only provide affordances for managing individual tabs and users who manage
their attention at the task level can lead to an &#8220;tab overload&#8221; problem. We
explored a task-centric tab manager called which enables users to save their
open tabs to manage them with task structures and affordances that better
reflect their mental models. To lower the cost of importing, Tabs.do uses a
neural network in the browser to make suggestions for grouping users&#8217; open tabs
by tasks.</p>

<!--more-->


<h2>Abstract</h2>

<p>The nature of people’s online activities has gone through dramatic changes in
past decades as significant portions of our productivity and sensemaking tasks
continue to migrate to “the cloud,” yet browser interfaces have stayed
largely the same since tabs were introduced nearly 20 years ago. The divide
between browser interfaces that only provide affordances for managing
individual tabs and users who manage their attention at the task level can lead
to serious adverse effects &#8211; commonly referred to as “tab overload.” This
paper explores the design of a task-centric tab manager called Tabs.do, which
enables users to import and close their open tabs to manage them as tasks.
Users can structure and prioritize their tasks to better reflect their mental
models and resume progress by reopening tasks into open tabs. To lower the cost
of importing, Tabs.do uses machine learning to make intelligent suggestions for
grouping users’ open tabs into task bundles with high precision by exploiting
behavioral and semantic features. Tabs.do bridges the gap between current
browser designs that treat tabs as stacks of independent webpages and users who
manage their workflow and attention at the tasks level. We conducted a field
deployment study where participants used Tabs.do with their real-life tasks and
tabs in the wild and uncovered insights around the costs, benefits, and
limitations of a task-centric approach to tab management.</p>

<h2>30 Seconds Preview (UIST 2021)</h2>

<iframe width="560" height="315" src="https://www.youtube.com/embed/he--Ly0UQ-4" frameborder="0" allowfullscreen></iframe>


<h2>Presentation (UIST 2021)</h2>

<iframe width="560" height="315" src="https://www.youtube.com/embed/ZwbVzDRFbGs" frameborder="0" allowfullscreen></iframe>


<h2>Downloads</h2>

<p><a class="btn btn-default" href="http://josephcc.github.com/images/papers/tabs.do.pdf" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'TabsDo', 'PDF']);" role="button">PDF Download</a>
<a class="btn btn-default" href="https://dl.acm.org/doi/10.1145/3472749.3474777" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'TabsDo', 'ACM']);"  role="button">ACM Digital Library</a></p>

<h2>Media Coverage</h2>

<ul>
<li>Mashable: <a href="https://mashable.com/article/too-many-tabs-open">Stop trying to work in multiple browser tabs. It&#8217;s terrible for your focus.</a></li>
<li>Fast Company: <a href="https://www.fastcompany.com/90635776/the-twisted-psychology-of-browser-tabs-and-why-we-cant-get-rid-of-them">The twisted psychology of browser tabs—and why we can&#8217;t get rid of them</a></li>
<li>MetroNews UK: <a href="https://metro.co.uk/2021/05/10/suffer-from-tab-overload-scientists-study-why-we-have-so-many-open-14540577/amp/">Suffer from &#8220;tab overload&#8221;? Scientists study why we have so many open</a></li>
<li>Inc.: <a href="https://www.inc.com/jessica-stillman/productivity-browser-tabs-carnegie-mellon.html">Stressed Out by Your 87 Open Browser Tabs? New Science Offers a Fix</a></li>
<li>HackerNews: <a href="https://news.ycombinator.com/item?id=27095701">When the Tab Comes Due: Challenges in the Cost Structure of Browser Tab Usage</a></li>
<li>HackerNews: <a href="https://news.ycombinator.com/item?id=27157225">Overcoming tab overload</a></li>
<li>Science Alert: <a href="https://www.sciencealert.com/tab-overload-is-a-common-problem-for-people-browsing-the-internet-survey-finds">We&#8217;re Getting Buried in Browser Tabs And Scientists Want to Fix It</a></li>
<li>EurekAlert: <a href="https://www.eurekalert.org/pub_releases/2021-05/cmu-oto050721.php">Overcoming tab overload</a></li>
<li>NewsGram: <a href="https://www.newsgram.com/skeema-this-tool-will-assist-you-in-managing-your-browser-tabs-more-effectively/">Skeema: This Tool Will Assist You In Managing Your Browser Tabs More Effectively</a></li>
<li>Sify: <a href="https://www.sify.com/news/this-tool-can-help-you-better-manage-browser-tabs-news-education-vfjl5Ebfaiifc.html">This tool can help you better manage browser tabs</a></li>
<li>Revyuh: <a href="https://www.revyuh.com/news/software/apps/browser-tabs-scientists-find-new-way-to-overcome-fear-of-black-hole-effect/">Browser Tabs: Scientists Find New Way to Overcome Fear of Black Hole Effect</a></li>
<li>Carnegie Mellon University: <a href="https://www.cmu.edu/news/stories/archives/2021/may/overcoming-tab-overload.html">Overcoming Tab Overload</a></li>
</ul>


<h2>Citation</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Joseph Chee Chang, Yongsung Kim, Victor Miller, Michael Xieyang Liu, Brad Myers, Aniket Kittur.
</span><span class='line'>Tabs.do: Task-Centric Browser Tab Management.
</span><span class='line'>In Proceedings of the 34th ACM User Interface Software and Technology Symposium: UIST 2021.</span></code></pre></td></tr></table></div></figure>


<h2>Bibtex</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>@article{chang2021tabsdo,
</span><span class='line'>  title={Tabs.do: Task-Centric Browser Tab Management.},
</span><span class='line'>  author={Chang, Joseph Chee and Kim, Yongsung and Miller, Victor and Liu, Michael Xieyang and Myers, Brad and Kittur, Aniket},
</span><span class='line'>  booktitle = {Proceedings of the 34th ACM User Interface Software and Technology Symposium},
</span><span class='line'>  series = {UIST'21},
</span><span class='line'>  year={2021}
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[When the Tab Comes Due]]></title>
    <link href="http://josephcc.github.com/CHI-browser-tabs/"/>
    <updated>2021-05-08T08:00:00-07:00</updated>
    <id>http://josephcc.github.com/CHI-browser-tabs</id>
    <content type="html"><![CDATA[<p><strong><em><i class="fa fa-trophy" aria-hidden="true"></i> BEST PAPER NOMINATION</em></strong>
<br/></p>

<p>Tabs have become integral to Web browsing yet have changed little since their
introduction nearly 20 years ago. In contrast, the internet has gone through
dramatic changes and increasingly used to support complex sensemaking tasks.
This paper investigates how tabs today are overloaded with a diverse set of
functionalities and issues users face when managing them. We uncovered
competing pressures pushing for keeping tabs open (ranging from interaction to
emotional costs) versus pushing for closing them (such as limited attention and
resources). We further developed rich design implications for future browser
interfaces.</p>

<!--more-->


<h2>Abstract</h2>

<p>Tabs have become integral to browsing the Web yet have changed little since
their introduction nearly 20 years ago. In contrast, the internet has gone
through dramatic changes, with users increasingly moving from navigating to
websites to exploring information across many sources to support online
sensemaking. This paper investigates how tabs today are overloaded with a
diverse set of functionalities and issues users face when managing them. We
interviewed ten information workers asking about their tab management
strategies and walk through each open tab on their work computers four times
over two weeks. We uncovered competing pressures pushing for keeping tabs open
(ranging from interaction to emotional costs) versus pushing for closing them
(such as limited attention and resources). We then surveyed 103 participants to
estimate the frequencies of these pressures at scale. Finally, we developed
design implications for future browser interfaces that can better support
managing these pressures</p>

<h2>Presentation (SIGCHI 2021)</h2>

<iframe width="560" height="315" src="https://www.youtube.com/embed/pBjIrX9H-Ns" frameborder="0" allowfullscreen></iframe>


<h2>Downloads</h2>

<p><a class="btn btn-default" href="http://josephcc.github.com/images/papers/tabs.pdf" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'TabHoarders', 'PDF']);" role="button">PDF Download</a>
<a class="btn btn-default" href="https://dl.acm.org/doi/10.1145/3411764.3445585" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'TabHoarders', 'ACM']);"  role="button">ACM Digital Library</a></p>

<h2>Media Coverage</h2>

<ul>
<li>Mashable: <a href="https://mashable.com/article/too-many-tabs-open">Stop trying to work in multiple browser tabs. It&#8217;s terrible for your focus.</a></li>
<li>Fast Company: <a href="https://www.fastcompany.com/90635776/the-twisted-psychology-of-browser-tabs-and-why-we-cant-get-rid-of-them">The twisted psychology of browser tabs—and why we can&#8217;t get rid of them</a></li>
<li>MetroNews UK: <a href="https://metro.co.uk/2021/05/10/suffer-from-tab-overload-scientists-study-why-we-have-so-many-open-14540577/amp/">Suffer from &#8220;tab overload&#8221;? Scientists study why we have so many open</a></li>
<li>Inc.: <a href="https://www.inc.com/jessica-stillman/productivity-browser-tabs-carnegie-mellon.html">Stressed Out by Your 87 Open Browser Tabs? New Science Offers a Fix</a></li>
<li>HackerNews: <a href="https://news.ycombinator.com/item?id=27095701">When the Tab Comes Due: Challenges in the Cost Structure of Browser Tab Usage</a></li>
<li>HackerNews: <a href="https://news.ycombinator.com/item?id=27157225">Overcoming tab overload</a></li>
<li>Science Alert: <a href="https://www.sciencealert.com/tab-overload-is-a-common-problem-for-people-browsing-the-internet-survey-finds">We&#8217;re Getting Buried in Browser Tabs And Scientists Want to Fix It</a></li>
<li>EurekAlert: <a href="https://www.eurekalert.org/pub_releases/2021-05/cmu-oto050721.php">Overcoming tab overload</a></li>
<li>NewsGram: <a href="https://www.newsgram.com/skeema-this-tool-will-assist-you-in-managing-your-browser-tabs-more-effectively/">Skeema: This Tool Will Assist You In Managing Your Browser Tabs More Effectively</a></li>
<li>Sify: <a href="https://www.sify.com/news/this-tool-can-help-you-better-manage-browser-tabs-news-education-vfjl5Ebfaiifc.html">This tool can help you better manage browser tabs</a></li>
<li>Revyuh: <a href="https://www.revyuh.com/news/software/apps/browser-tabs-scientists-find-new-way-to-overcome-fear-of-black-hole-effect/">Browser Tabs: Scientists Find New Way to Overcome Fear of Black Hole Effect</a></li>
<li>Carnegie Mellon University: <a href="https://www.cmu.edu/news/stories/archives/2021/may/overcoming-tab-overload.html">Overcoming Tab Overload</a></li>
</ul>


<h2>Citation</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Joseph Chee Chang, Nathan Hahn, Yongsung Kim, Julina Coupland, Bradley
</span><span class='line'>Breneisen, Hannah S Kim, John Hwong, Aniket Kittur. 2021.
</span><span class='line'>When the Tab Comes Due: Challenges in the Cost Structure of Browser Tab Usage.
</span><span class='line'>In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (CHI '21).
</span><span class='line'>ACM, New York, NY, USA, 15 pages. DOI: http://dx.doi.org/10.1145/</span></code></pre></td></tr></table></div></figure>


<h2>Bibtex</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>@inproceedings{Chang:tabs:2021,
</span><span class='line'> author = {Chang, Joseph Chee and Hahn, Nathan and Kim, Yongsung and Coupland,
</span><span class='line'> Julina and Breneisen, Bradley and Kim, Hannah S and Hwong, John and Kittur,
</span><span class='line'> Aniket},
</span><span class='line'> title = {When the Tab Comes Due: Challenges in the Cost Structure of Browser Tab Usage},
</span><span class='line'> booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
</span><span class='line'> series = {CHI '21},
</span><span class='line'> year = {2021},
</span><span class='line'> location = {Online},
</span><span class='line'> numpages = {15},
</span><span class='line'> url = {http://doi.acm.org/10.1145/3411764.3445585},
</span><span class='line'> doi = {10.1145/3411764.3445585},
</span><span class='line'> acmid = {3445585},
</span><span class='line'> publisher = {ACM},
</span><span class='line'> address = {New York, NY, USA},
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mesh]]></title>
    <link href="http://josephcc.github.com/UIST-mesh/"/>
    <updated>2020-10-20T08:00:00-07:00</updated>
    <id>http://josephcc.github.com/UIST-mesh</id>
    <content type="html"><![CDATA[<p>Consumers can choose from many different products and base their decisions on
the tens of thousands of online evidence about each of their options.  However,
to synthesize this information into confident decisions can incur high
interaction and cognitive costs. Online information is scattered across
different sources, and evidence such as reviews can be subjective and
conflicting, requiring users to interpret them under their personal context. We
introduce Mesh, which scaffolds users in iteratively building up a better
understanding of both their choices by evaluating evidence gathered across
sources. Lab and field deployment studies found that Mesh significantly reduces
the costs of gathering and evaluating evidence and scaffolds decision-making
through personalized criteria enabling users to gain deeper insights from data
to make confident purchase decisions.</p>

<!--more-->


<h2>Abstract</h2>

<p>While there is an enormous amount of information online for making decisions
such as choosing a product, restaurant, or school, it can be costly for users
to synthesize that information into confident decisions. Information for users&#8217;
many different criteria needs to be gathered from many different sources into a
structure where they can be compared and contrasted. The usefulness of each
criterion for differentiating potential options can be opaque to users, and
evidence such as reviews may be subjective and conflicting, requiring users to
interpret each under their personal context. We introduce Mesh, which
scaffolds users in iteratively building up a better understanding of both their
criteria and options by evaluating evidence gathered across sources in the
context of consumer decision making. Mesh bridges the gap between decision
support systems that typically have rigid structures and the fluid and dynamic
process of exploratory search, changing the cost structure to provide
increasing payoffs with greater user investment. Our lab and field deployment
studies found evidence that Mesh significantly reduces the costs of
gathering and evaluating evidence and scaffolds decision-making through
personalized criteria enabling users to gain deeper insights from data.</p>

<h2>Video Figure</h2>

<h4>5-minute virtual conference presentation</h4>

<iframe width="560" height="315" src="https://www.youtube.com/embed/LNASh9rq9-I?rel=0" frameborder="0" allowfullscreen></iframe>


<h4>3-minute video abstract</h4>

<iframe width="560" height="315" src="https://www.youtube.com/embed/NqriHlTfVhU?rel=0" frameborder="0" allowfullscreen></iframe>


<h2>Download</h2>

<p><a class="btn btn-default" href="http://josephcc.github.com/images/papers/mesh.pdf" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Mesh', 'PDF']);" role="button">PDF Download</a>
<a class="btn btn-default" href="https://dl.acm.org/doi/10.1145/3379337.3415865" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Mesh', 'ACM']);"  role="button">ACM Digital Library</a></p>

<h2>Citation</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Joseph Chee Chang, Nathan Hahn, Aniket Kittur.
</span><span class='line'>Mesh: Scaffolding Comparison Tables for Online Decision Making.
</span><span class='line'>In Proceedings of the 33rd ACM User Interface Software and Technology Symposium: UIST 2020.</span></code></pre></td></tr></table></div></figure>


<h2>Bibtex</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>@article{chang2020mesh,
</span><span class='line'>  title={Mesh: Scaffolding Comparison Tables for Online Decision Making},
</span><span class='line'>  author={Chang, Joseph Chee and Hahn, Nathan, and Kittur, Aniket},
</span><span class='line'>  booktitle = {Proceedings of the 33rd ACM User Interface Software and Technology Symposium},
</span><span class='line'>  series = {UIST'20},
</span><span class='line'>  year={2020}
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ph.D Thesis]]></title>
    <link href="http://josephcc.github.com/CMU-thesis/"/>
    <updated>2020-06-02T08:00:00-07:00</updated>
    <id>http://josephcc.github.com/CMU-thesis</id>
    <content type="html"><![CDATA[<!--more-->


<p><a class="btn btn-default" href="http://josephcc.github.com/Thesis.pdf" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Thesis', 'PDF']);" role="button">PDF Download</a></p>

<iframe src="http://josephcc.github.com/Thesis.pdf" style='width: 100%; height: 800px; border: 1px darkgray solid;'>
  Thesis
</iframe>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SearchLens]]></title>
    <link href="http://josephcc.github.com/IUI-searchlens/"/>
    <updated>2019-03-17T08:00:00-07:00</updated>
    <id>http://josephcc.github.com/IUI-searchlens</id>
    <content type="html"><![CDATA[<p>Whether figuring out where to eat in an unfamiliar city or deciding which
apartment to live in, reviews and forum posts are often a significant factor in
online decision making. However, making sense of these rich repositories of
diverse opinions can be prohibitively effortful, searchers need to sift through
a large number of reviews to characterize each item based on aspects that they
care about. We introduce a novel system, SearchLens, where searchers build up a
collection of composable and reusable &#8220;Lenses&#8221; that reflect their different
latent interests. Also, the Lenses allowed the system to generate personalized
interfaces with visual explanations that promote transparency and enable
in-depth exploration.</p>

<!--more-->


<h2>Abstract</h2>

<p>Whether figuring out where to eat in an unfamiliar city or deciding which
apartment to live in, consumer generated data (i.e. reviews and forum posts)
are often an important influence in online decision making. To make sense of
these rich repositories of diverse opinions, searchers need to sift through a
large number of reviews to characterize each item based on aspects that they
care about. We introduce a novel system, SearchLens, where searchers build up a
collection of &#8220;Lenses&#8221; that reflect their different latent interests, and
compose the Lenses to find relevant items across different contexts. Based on
the Lenses, SearchLens generates personalized interfaces with visual
explanations that promotes transparency and enables deeper exploration. While
prior work found searchers may not wish to put in effort specifying their goals
without immediate and sufficient benefits, results from a controlled lab study
suggest that our approach incentivized participants to express their interests
more richly than in a baseline condition, and a field study showed that
participants found benefits in SearchLens while conducting their own tasks.</p>

<h2>Video Figure</h2>

<iframe width="560" height="315" src="https://www.youtube.com/embed/dXcTtHMa2DQ?rel=0" frameborder="0" allowfullscreen></iframe>


<h2>Download</h2>

<p><a class="btn btn-default" href="http://josephcc.github.com/images/papers/searchlens.pdf" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'SearchLens', 'PDF']);" role="button">PDF Download</a>
<a class="btn btn-default" href="https://dl.acm.org/citation.cfm?id=3302321" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'SearchLens', 'ACM']);"  role="button">ACM Digital Library</a></p>

<h2>Citation</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>SearchLens: Composing and Capturing Complex User Interests for Exploratory Search.
</span><span class='line'>Joseph Chee Chang, Nathan Hahn, Adam Perer, Aniket Kittur.
</span><span class='line'>In Proceedings of the 2019 ACM 24th Annual Meeting of the Intelligent User Interfaces: IUI 2019.</span></code></pre></td></tr></table></div></figure>


<h2>Bibtex</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>@article{chang2019searchlens,
</span><span class='line'>  title={SearchLens: Composing and Capturing Complex User Interests for Exploratory Search},
</span><span class='line'>  author={Chang, Joseph Chee and Hahn, Nathan, and Perer, Adam and Kittur, Aniket},
</span><span class='line'>  booktitle = {Proceedings of the 2019 ACM 24th Annual Meeting of the Intelligent User Interfaces},
</span><span class='line'>  series = {IUI'19},
</span><span class='line'>  year={2019}
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Solvent]]></title>
    <link href="http://josephcc.github.com/CSCW-solvent/"/>
    <updated>2018-11-03T08:00:00-07:00</updated>
    <id>http://josephcc.github.com/CSCW-solvent</id>
    <content type="html"><![CDATA[<p>Analogies in distant domains often lead to scientific discoveries. However, it
can be prohibitively difficult for researchers to find useful analogies from
unfamiliar domains as search engines poorly support it. We introduce Solvent, a
mixed-initiative system where annotators structure abstracts of academic papers
into different aspects and use a semantic model to find analogies among
research papers and across different domains. These results demonstrate a new
path towards computationally supported knowledge sharing in research
communities.</p>

<!--more-->


<h2>Abstract</h2>

<p>Scientific discoveries are often driven by finding analogies in distant
domains, but the growing number of papers makes it difficult to find relevant
ideas in a single discipline, let alone distant analogies in other domains. To
provide computational support for finding analogies across domains, we
introduce Solvent, a mixed-initiative system where humans annotate aspects of
research papers that denote their background (the high-level problems being
addressed), purpose (the specific problems being addressed), mechanism (how
they achieved their purpose), and findings (what they learned/achieved), and a
computational model constructs a semantic representation from these annotations
that can be used to find analogies among the research papers. We demonstrate
that this system finds more analogies than baseline information-retrieval
approaches; that annotators and annotations can generalize beyond domain; and
that the resulting analogies found are useful to experts. These results
demonstrate a novel path towards computationally supported knowledge sharing in
research communities.</p>

<h2>Downloads</h2>

<p><a class="btn btn-default" href="http://josephcc.github.com/images/papers/solvent.pdf" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Solvent', 'PDF']);" role="button">PDF Download</a></p>

<h2>Citation</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Joel Chan, Joseph Chee Chang, Tom Hope, Dafna Shahaf, Aniket Kittur. 2018.
</span><span class='line'>Solvent: A Mixed Initiative System for Finding Analogies between Research Papers
</span><span class='line'>In Proceedings of the 2018 ACM Human-Computer Interaction: CSCW 2018.</span></code></pre></td></tr></table></div></figure>


<h2>Bibtex</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>@article{chan2018solvent,
</span><span class='line'>  title={SOLVENT: A Mixed Initiative System for Finding Analogies between Research Papers},
</span><span class='line'>  author={CHAN, JOEL and CHANG, JOSEPH CHEE and HOPE, TOM and SHAHAF, DAFNA and KITTUR, ANIKET},
</span><span class='line'>  booktitle = {Proceedings of the 2018 ACM Human-Computer Interaction: CSCW},
</span><span class='line'>  series = {CSCW'18},
</span><span class='line'>  year={2018}
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Bento Browser]]></title>
    <link href="http://josephcc.github.com/CHI-bento/"/>
    <updated>2018-04-21T08:01:00-07:00</updated>
    <id>http://josephcc.github.com/CHI-bento</id>
    <content type="html"><![CDATA[<p>Complex searches can be overwhelming, leading to lots of opened tabs. This tab overload can
make conducting searches on mobile devices especially difficult where screen
real-estate is limited, and progress can often be interrupted. Rather than
using tabs to manage information, we introduce browsing through scaffolding.
Search result lists serve as mutable workspaces where progress can be suspended
and resumed. BentoBrowser is available for <a href="https://itunes.apple.com/us/app/bento-browser/id1101530325?mt=8">download from the iPhone AppStore</a>.</p>

<!--more-->


<h2>Abstract</h2>

<p>People engaged in complex searches such as planning a vacation or understanding
their medical symptoms are often overwhelmed by opening and managing many tabs.
These challenges are exacerbated as search moves to smartphones and mobile
devices where screen real-estate is limited and tasks are frequently suspended,
resumed, and interleaved. Rather than continue to utilize tab-based browsing
for complex search, we introduce a new way of browsing through a scaffolded
interface. The list of search results serves as a mutable workspace, where a
user can track progress on a specific information query. The search query
serves as a gateway into this workspace, accessed through a task-subtask
hierarchy. We instantiate this in the Bento mobile search system and
investigate its effectiveness in three studies. We find converging evidence
that users were able to make progress on their complex searching tasks with
this structure, and find it more organized and easier to revisit.</p>

<h2>Install Bento Browser</h2>

<p>The version on the AppStore is the third iteration of the version from the CHI 2018 paper.</p>

<p><img src="http://josephcc.github.com/images/projects/bento_screenshots.png" style='width: 100%;'/></p>

<p><a class="btn btn-default" href="https://itunes.apple.com/us/app/bento-browser/id1101530325?mt=8" target='_blank' onclick="_gaq.push(['_trackEvent', 'Demo', 'Bento', 'AppStore']);" role="button">iPhone App (AppStore)</a></p>

<h2>Video Demo</h2>

<p>The version in the video is the second iteration of the version from the CHI 2018 paper.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/fm5zK1vm1Q8?rel=0" frameborder="0" allowfullscreen></iframe>


<h2>Media Coverage</h2>

<ul>
<li>Tech Explorist: <a href="https://www.techexplorist.com/new-web-browser-easier-search-mobile-devices/13794/">A new web browser makes it easier to search on mobile devices</a></li>
<li>THE Journal: <a href="https://thejournal.com/articles/2018/05/02/carnegie-mellons-bento-browser-organizes-complex-mobile-searches.aspx">New Bento Browser Organizes Complex Mobile Searches</a></li>
<li>Science Daily: <a href="https://www.sciencedaily.com/releases/2018/04/180425093747.htm">Bento browser makes it easier to search on mobile devices</a></li>
<li>Prikk: <a href="https://www.prikk.world/en/news/wirtschaft/innovation/bento-erleichtert-websuche-auf-mobilgeraten">&#8220;BENTO&#8221; ERLEICHTERT WEBSUCHE AUF MOBILGERÄTEN</a></li>
<li>CMU News: <a href="https://www.cmu.edu/news/stories/archives/2018/april/bento-browser.html">Bento Browser Makes it Easier To Search on Mobile Devices</a></li>
<li>CMU HCII: <a href="https://hcii.cmu.edu/news/2018/bento-browser-makes-it-easier-search-mobile-devices">Bento Browser Makes it Easier To Search on Mobile Devices</a></li>
</ul>


<h2>Downloads</h2>

<p><a class="btn btn-default" href="http://josephcc.github.com/images/papers/bento.pdf" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Evorus', 'PDF']);" role="button">PDF Download</a></p>

<h2>Citation</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Nathan Hahn, Joseph Chee Chang, Aniket Kittur. 2018.
</span><span class='line'>Bento Browser: Complex Mobile Search Without Tabs.
</span><span class='line'>In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI '18).
</span><span class='line'>ACM, Montreal, QC, Canada.</span></code></pre></td></tr></table></div></figure>


<h2>Bibtex</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>@inproceedings{Hahn:2018:Bento,
</span><span class='line'> author = {Hahn, Nathan and Chang, Joseph Chee and Kittur, Aniket},
</span><span class='line'> title = {Bento Browser: Complex Mobile Search Without Tabs.},
</span><span class='line'> booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
</span><span class='line'> series = {CHI '18},
</span><span class='line'> year = {2018},
</span><span class='line'> publisher = {ACM},
</span><span class='line'> address = {Montreal, QC, Canada},
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Evorus]]></title>
    <link href="http://josephcc.github.com/CHI-evorus/"/>
    <updated>2018-04-21T08:00:00-07:00</updated>
    <id>http://josephcc.github.com/CHI-evorus</id>
    <content type="html"><![CDATA[<p><strong><em><i class="fa fa-trophy" aria-hidden="true"></i> BEST PAPER NOMINATION</em></strong>
<br/>
Crowd-powered chatbots are robust than current pure AI approach, but can be
slower and more expensive at runtime. We attempted to combine the two
approaches for high quality, low latency, and low cost.  We introduce Evorus, a
crowd-powered chatbot that automate itself over time by learning to integrate
AI chatbots, reusing responses, and assess response quality. A 5-month-long
public deployment study shows promising results. You can <a href="http://talkingtothecrowd.org/">try talking to Evorus today</a>.</p>

<!--more-->


<h2>Abstract</h2>

<p>Crowd-powered conversational assistants have been shown to be more robust than
automated systems, but do so at the cost of higher response latency and
monetary costs.  A promising direction is to combine the two approaches for
high quality, low latency, and low cost solutions.  In this paper, we introduce
Evorus, a crowd-powered conversational assistant built to automate itself over
time by i) allowing new chatbots to be easily integrated to automate more
scenarios, ii) reusing prior crowd answers, and iii) learning to automatically
approve response candidates.  Our 5-month-long deployment with 80 participants
and 281 conversations shows that Evorus can automate itself without
compromising conversation quality.  Crowd-AI architectures have long been
proposed as a way to reduce cost and latency for crowd-powered systems; Evorus
demonstrates how automation can be introduced successfully in a deployed
system. Its architecture allows future researchers to make further innovation
on the underlying automated components in the context of a deployed open domain
dialog system.</p>

<h2>Try Evorus on Google Talk</h2>

<p><a class="btn btn-default" href="http://talkingtothecrowd.org/" target='_blank' onclick="_gaq.push(['_trackEvent', 'Demo', 'Evorus', 'Website']);" role="button">TalkingToTheCrowd.org</a></p>

<h2>Overview Video</h2>

<iframe width="560" height="315" src="https://www.youtube.com/embed/3SAG8jP-Q-M?rel=0" frameborder="0" allowfullscreen></iframe>


<h2>Media Coverage</h2>

<ul>
<li>WTAE TV News: <a href="http://www.wtae.com/article/chorus-chatbot-carnegie-mellon-university-pittsburgh/16870459">Meet the &#8216;Chorus&#8217; chatbot: Unlike Alexa or Siri, it&#8217;s powered by actual people on the other end</a></li>
<li>Trib Live: <a href="http://triblive.com/business/technology/13275597-74/chatbot-developed-at-carnegie-mellon-uses-humans-to-answer-questions-ais-cant">Chatbot developed at Carnegie Mellon uses humans to answer questions AIs can&#8217;t</a></li>
<li>EurekaAlert!: <a href="https://www.eurekalert.org/pub_releases/2018-02/cmu-cwa020618.php">Crowd workers, AI make conversational agents smarter</a></li>
<li>CMU News: <a href="https://www.cs.cmu.edu/news/crowd-workers-ai-make-conversational-agents-smarter">Crowd workers, AI make conversational agents smarter</a></li>
<li>Presstext: <a href="https://www.pressetext.com/#news/20180208014">KI-System &#8220;Evorus&#8221; wird zunehmend eigenständig</a></li>
</ul>


<h2>Downloads</h2>

<p><a class="btn btn-default" href="http://josephcc.github.com/images/papers/evorus.pdf" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Evorus', 'PDF']);" role="button">PDF Download</a>
<a class="btn btn-default" href="https://arxiv.org/abs/1801.02668" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Evorus', 'arXiv']);" role="button">arXiv</a></p>

<h2>Citation</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Kenneth Huang, Joseph Chee Chang, Jeffrey P. Bigham. 2018.
</span><span class='line'>Evorus: A Crowd-powered Conversational AssistantBuilt to Automate Itself Over Time.
</span><span class='line'>In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI '18).
</span><span class='line'>ACM, Montreal, QC, Canada.</span></code></pre></td></tr></table></div></figure>


<h2>Bibtex</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>@inproceedings{Huang:2018:Evorus,
</span><span class='line'> author = {Huang, Kenneth and Chang, Joseph Chee and Bigham, Jeffrey P.},
</span><span class='line'> title = {Evorus: A Crowd-powered Conversational AssistantBuilt to Automate Itself Over Time},
</span><span class='line'> booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
</span><span class='line'> series = {CHI '18},
</span><span class='line'> year = {2018},
</span><span class='line'> publisher = {ACM},
</span><span class='line'> address = {Montreal, QC, Canada},
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Revolt]]></title>
    <link href="http://josephcc.github.com/CHI-revolt/"/>
    <updated>2017-05-06T08:00:00-07:00</updated>
    <id>http://josephcc.github.com/CHI-revolt</id>
    <content type="html"><![CDATA[<p>Generating comprehensive labeling guidelines for crowdworkers can be
challenging for complex datasets.  Revolt harnesses <em>crowd disagreements</em>
to identify ambiguous concepts in the data and coordinates the crowd to
<em>collaboratively</em> create rich structures for requesters to make posthoc
decisions, removing the need for comprehensive guidelines and
enabling dynamic label boundaries.</p>

<p><span style='color: gray'>Work done during internship at Microsoft Research, Redmond.</span></p>

<!--more-->


<h2>Abstract</h2>

<p>Crowdsourcing provides a scalable and efficient way to construct labeled
datasets for training machine learning systems. However, creating comprehensive
label guidelines for crowdworkers is often prohibitive even for seemingly
simple concepts. Incomplete or ambiguous label guidelines can then result in
differing interpretations of concepts and inconsistent labels. Existing
approaches for improving label quality, such as worker screening or detection
of poor work, are ineffective for this problem and can lead to rejection of
honest work and a missed opportunity to capture rich interpretations about
data. We introduce Revolt, a collaborative approach that brings ideas from
expert annotation workflows to crowd-based labeling. Revolt eliminates the
burden of creating detailed label guidelines by harnessing crowd disagreements
to identify ambiguous concepts and create rich structures (groups of
semantically related items) for post-hoc label decisions. Experiments comparing
Revolt to traditional crowdsourced labeling show that Revolt produces high
quality labels without requiring label guidelines in turn for an increase in
monetary cost. This up front cost, however, is mitigated by Revolt&#8217;s ability to
produce reusable structures that can accommodate a variety of label boundaries
without requiring new data to be collected. Further comparisons of Revolt&#8217;s
collaborative and non-collaborative variants show that collaboration reaches
higher label accuracy with lower monetary cost.</p>

<h2>Downloads</h2>

<p><a class="btn btn-default" href="http://josephcc.github.com/images/papers/revolt-crowd-labeling.pdf" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Revolt', 'PDF']);" role="button">PDF Download</a>
<a class="btn btn-default" href="http://dl.acm.org/citation.cfm?id=3026044" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Revolt', 'ACM']);" role="button">ACM Digital Library</a>
<a class="btn btn-default" href="http://josephcc.github.com/images/papers/revolt-notes.pdf" target='_blank' onclick="_gaq.push(['_trackEvent', 'Notes', 'Revolt', 'PDF']);" role="button">Technical and Design Notes (draft)</a></p>

<h2>Citation</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Joseph Chee Chang, Saleema Amershi, and Ece Kamar. 2017.
</span><span class='line'>Revolt: Collaborative Crowdsourcing for Labeling Machine Learning Datasets.
</span><span class='line'>In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI '17).
</span><span class='line'>ACM, New York, NY, USA, 3180-3191. DOI: http://dx.doi.org/10.1145/3025453.3026044</span></code></pre></td></tr></table></div></figure>


<h2>Bibtex</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>@inproceedings{Chang:2017:Revolt,
</span><span class='line'> author = {Chang, Joseph Chee and Amershi, Saleema and Kamar, Ece},
</span><span class='line'> title = {Revolt: Collaborative Crowdsourcing for Labeling Machine Learning Datasets},
</span><span class='line'> booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
</span><span class='line'> series = {CHI '17},
</span><span class='line'> year = {2017},
</span><span class='line'> url = {http://doi.acm.org/10.1145/3025453.3026044},
</span><span class='line'> doi = {10.1145/3025453.3026044},
</span><span class='line'> publisher = {ACM},
</span><span class='line'> address = {New York, NY, USA},
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Intentionally Uncertain Input]]></title>
    <link href="http://josephcc.github.com/UIST-highlight/"/>
    <updated>2016-10-16T08:00:00-07:00</updated>
    <id>http://josephcc.github.com/UIST-highlight</id>
    <content type="html"><![CDATA[<p>Highlighting can be mentally taxing for learners who are often unsure about how
much information they needed to include.  We introduce the idea of
intentionally uncertain input in the context of highlighting on mobile devices.
We present a system that uses force touch and fuzzy bounding boxes to support
saving information while users are uncertain about where to highlight.</p>

<!--more-->


<h2>Abstract</h2>

<p>Patients researching medical diagnoses, scientist exploring new fields of
literature, and students learning about new domains are all faced with the
challenge of capturing information they find for later use. However, saving
information is challenging on mobile devices, where the small screen and font
sizes combined with the inaccuracy of finger based touch screens makes it time
consuming and stressful for people to select and save text for future use.
Furthermore, beyond the challenge of simply selecting a region of bounded text
on a mobile device, in many learning and data exploration tasks the boundaries
of what text may be relevant and useful later are themselves uncertain for the
user. In contrast to previous approaches which focused on speeding up the
selection process by making the identification of hard boundaries faster, we
introduce the idea of intentionally supporting uncertain input in the context
of saving information during complex reading and information exploration. We
embody this idea in a system that uses force touch and fuzzy bounding boxes
along with posthoc expandable context to support identifying and saving
information in an intentionally uncertain way on mobile devices. In a two part
user study we find that this approach reduced selection time and was preferred
by participants over the default system text selection method.</p>

<h2>Presentation (UIST 2016)</h2>

<iframe width="560" height="315" src="https://www.youtube.com/embed/HPG6NWRTGvY?list=PLqhXYFYmZ-VcUPus2QYpAZdFmw5w6seVR" frameborder="0" allowfullscreen></iframe>


<h2>Demo Video</h2>

<iframe width="560" height="315" src="https://www.youtube.com/embed/Rj_0GFeUQjg?rel=0" frameborder="0" allowfullscreen></iframe>


<h2>Downloads</h2>

<p><a class="btn btn-default" href="http://josephcc.github.com/images/papers/mobile-highlighting.pdf" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Highlight', 'PDF']);" role="button">PDF Download</a>
<a class="btn btn-default" href="http://dl.acm.org/citation.cfm?id=2984538" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Highlight', 'ACM']);"  role="button">ACM Digital Library</a></p>

<h2>Citation</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Joseph Chee Chang, Nathan Hahn, and Aniket Kittur. 2016.
</span><span class='line'>Supporting Mobile Sensemaking Through Intentionally Uncertain Highlighting.
</span><span class='line'>In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST '16).
</span><span class='line'>ACM, New York, NY, USA, 61-68. DOI: http://dx.doi.org/10.1145/2984511.2984538</span></code></pre></td></tr></table></div></figure>


<h2>Bibtex</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>@inproceedings{Chang:2016:SMS:2984511.2984538,
</span><span class='line'> author = {Chang, Joseph Chee and Hahn, Nathan and Kittur, Aniket},
</span><span class='line'> title = {Supporting Mobile Sensemaking Through Intentionally Uncertain Highlighting},
</span><span class='line'> booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
</span><span class='line'> series = {UIST '16},
</span><span class='line'> year = {2016},
</span><span class='line'> isbn = {978-1-4503-4189-9},
</span><span class='line'> location = {Tokyo, Japan},
</span><span class='line'> pages = {61--68},
</span><span class='line'> numpages = {8},
</span><span class='line'> url = {http://doi.acm.org/10.1145/2984511.2984538},
</span><span class='line'> doi = {10.1145/2984511.2984538},
</span><span class='line'> acmid = {2984538},
</span><span class='line'> publisher = {ACM},
</span><span class='line'> address = {New York, NY, USA},
</span><span class='line'> keywords = {annotation, capture, copy-paste, highlighting., information, saving, sensemaking},
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Alloy]]></title>
    <link href="http://josephcc.github.com/CHI-alloy/"/>
    <updated>2016-05-07T08:00:00-07:00</updated>
    <id>http://josephcc.github.com/CHI-alloy</id>
    <content type="html"><![CDATA[<p><strong><em><i class="fa fa-trophy" aria-hidden="true"></i> BEST PAPER NOMINATION</em></strong>
<br/>
<strong><em>HCOMP 2016 INVITED ENCORE TALK</em></strong>
<br/>
Many crowd clustering approaches have difficulties providing global context to
workers in order to generate meaningful categories. Alloy uses a
<em>sample-and-search</em> technique to provide a better understanding of the global
context. It also combines the in-depth semantic knowledge from human
computation and the scalability of machine learning models to create rich
structures from unorganized documents with high quality and efficiency.</p>

<!--more-->


<h2>Abstract</h2>

<p>Crowdsourced clustering approaches present a promising way to harness deep
semantic knowledge for clustering complex information. However, existing
approaches have difficulties supporting the global context needed for workers
to generate meaningful categories, and are costly because all items require
human judgments. We introduce Alloy, a hybrid approach that combines the
richness of human judgments with the power of machine algorithms. Alloy
supports greater global context through a new <em>sample and search</em>
crowd pattern which changes the crowd&#8217;s task from classifying a fixed subset of
items to actively sampling and querying the entire dataset.  It also improves
efficiency through a two phase process in which crowds provide examples to help
a machine cluster the head of the distribution, then classify low-confidence
examples in the tail. To accomplish this, Alloy introduces a modular
<em>cast and gather</em> approach which leverages a machine learning backbone
to stitch together different types of judgment tasks.</p>

<h2>Downloads</h2>

<p><a class="btn btn-default" href="http://josephcc.github.com/images/papers/alloy-crowd-clustering.pdf" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Alloy', 'PDF']);" role="button">PDF Download</a>
<a class="btn btn-default" href="http://dl.acm.org/citation.cfm?id=2858411" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Alloy', 'ACM']);" role="button">ACM Digital Library</a></p>

<h2>Media Coverage</h2>

<ul>
<li>Pittsburgh Post-Gazette: <a href="http://www.post-gazette.com/business/tech-news/2016/05/11/Crowdsourcing-work-Get-rid-of-the-human-supervisor-CMU/stories/201605100127">Crowdsourcing work? Get rid of the human supervisor</a></li>
<li>Campus Technology: <a href="https://campustechnology.com/articles/2016/05/10/research-project-mixes-humans-and-machines-for-better-crowdsourcing.aspx">Research Project Mixes Humans and Machines for Better Crowdsourcing</a></li>
<li>Neuraoscience News: <a href="http://neurosciencenews.com/human-machine-intelligence-framework-4221/">Crowd Augmented Cognition: Combining Human and Machine Intelligence to Accelerate Learning</a></li>
<li>DZone: <a href="https://dzone.com/articles/researchers-work-on-automated-means-of-managing-th">Research Suggests AI Managers Effective for Crowdsourcing</a></li>
<li>PhysOrg: <a href="https://phys.org/news/2016-05-crowd-augmented-cognition-team-tools-combine.html">Crowd-augmented cognition: Team develops tools that combine human and machine intelligence to accelerate learning</a></li>
<li>TechExplore: <a href="https://techxplore.com/news/2016-05-big-small-pieces-humans-crowdsourced.html">Big thinking in small pieces: Computer guides humans in crowdsourced research</a></li>
<li>Spend Matters: <a href="http://spendmatters.com/2016/06/09/crowdsourcing-and-cognitive-computing-are-you-ready-for-the-future-of-work/">Crowdsourcing and Cognitive Computing: Are You Ready for the Future of Work?</a></li>
<li>Science Daily: <a href="https://www.sciencedaily.com/releases/2016/05/160511210628.htm">Crowd-augmented cognition - combine human, machine intelligence to accelerate learning</a></li>
<li>EurekAlert: <a href="https://www.eurekalert.org/pub_releases/2016-05/cmu-bti051016.php">Big thinking in small pieces: Computer guides humans in crowdsourced research</a></li>
<li>EurekAlert: <a href="https://www.eurekalert.org/pub_releases/2016-05/nsf-cc051116.php">Crowd-augmented cognition</a></li>
<li>NSF News: <a href="https://www.nsf.gov/news/mmg/mmg_disp.jsp?med_id=80586&amp;from=">Big thinking in small pieces: Computer guides humans in crowdsourced research</a></li>
<li>NSF News: <a href="https://www.nsf.gov/news/news_summ.jsp?cntn_id=138580&amp;org=NSF">Crowd-augmented cognition</a></li>
<li>CMU SCS News: <a href="https://www.cmu.edu/news/stories/archives/2016/may/knowledge-accelerator.html">Big thinking in small pieces: Computer guides humans in crowdsourced research</a></li>
<li>CMU HCII News: <a href="https://hcii.cmu.edu/news/2016/hcii-chi-computer-guides-humans-crowdsourced-research">Computer Guides Humans in Crowdsourced Research</a></li>
</ul>


<h2>Citation</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Joseph Chee Chang, Aniket Kittur, and Nathan Hahn. 2016.
</span><span class='line'>Alloy: Clustering with Crowds and Computation.
</span><span class='line'>In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI '16).
</span><span class='line'>ACM, New York, NY, USA, 3180-3191. DOI: http://dx.doi.org/10.1145/2858036.2858411</span></code></pre></td></tr></table></div></figure>


<h2>Bibtex</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>@inproceedings{Chang:2016:ACC:2858036.2858411,
</span><span class='line'> author = {Chang, Joseph Chee and Kittur, Aniket and Hahn, Nathan},
</span><span class='line'> title = {Alloy: Clustering with Crowds and Computation},
</span><span class='line'> booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
</span><span class='line'> series = {CHI '16},
</span><span class='line'> year = {2016},
</span><span class='line'> isbn = {978-1-4503-3362-7},
</span><span class='line'> location = {Santa Clara, California, USA},
</span><span class='line'> pages = {3180--3191},
</span><span class='line'> numpages = {12},
</span><span class='line'> url = {http://doi.acm.org/10.1145/2858036.2858411},
</span><span class='line'> doi = {10.1145/2858036.2858411},
</span><span class='line'> acmid = {2858411},
</span><span class='line'> publisher = {ACM},
</span><span class='line'> address = {New York, NY, USA},
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Knowledge Accelorator]]></title>
    <link href="http://josephcc.github.com/CHI-ka/"/>
    <updated>2016-05-07T07:00:00-07:00</updated>
    <id>http://josephcc.github.com/CHI-ka</id>
    <content type="html"><![CDATA[<p><strong><em><i class="fa fa-trophy" aria-hidden="true"></i> BEST PAPER NOMINATION</em></strong>
<br/>
Answering complex questions such as &#8220;How do I grow better tomatoes?&#8221; often
requires individuals to conduct extensive online research and synthesis. Can we
crowdsource this complex, high context process with 100 crowdworkers conducting
microtasks distributedly? The Knowledge Accelerator uses crowdworkers to
extract and synthesize text clips across web pages into coherent articles
without a centralized coordinator.</p>

<!--more-->


<h2>Abstract</h2>

<p>Crowdsourcing  offers  a  powerful  new  paradigm  for  onlinework.   However,
real  world  tasks  are  often  interdependent,requiring a big picture view of
the difference pieces involved. Existing  crowdsourcing  approaches  that
support  such  tasks &#8211; ranging from Wikipedia to flash teams &#8211; are
bottleneckedby relying on a small number of individuals to maintain thebig
picture.   In this paper,  we explore the idea that a computational system can
scaffold an emerging interdependent,big picture view entirely through the small
contributions ofindividuals, each of whom sees only a part of the whole.
Toinvestigate the viability, strengths, and weaknesses of this approach we
instantiate the idea in a prototype system for accomplishing  distributed
information  synthesis  and  evaluateits output across a variety of topics.  We
also contribute a setof design patterns that may be informative for other
systemsaimed at supporting big picture thinking in small pieces.</p>

<h2>Downloads</h2>

<p><a class="btn btn-default" href="http://josephcc.github.com/images/papers/knowledge-accelorator.pdf" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'KA', 'PDF']);" role="button">PDF Download</a>
<a class="btn btn-default" href="http://dl.acm.org/citation.cfm?id=2858364" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'KA', 'ACM']);"  role="button">ACM Digital Library</a></p>

<h2>Media Coverage</h2>

<ul>
<li>Pittsburgh Post-Gazette: <a href="http://www.post-gazette.com/business/tech-news/2016/05/11/Crowdsourcing-work-Get-rid-of-the-human-supervisor-CMU/stories/201605100127">Crowdsourcing work? Get rid of the human supervisor</a></li>
<li>Campus Technology: <a href="https://campustechnology.com/articles/2016/05/10/research-project-mixes-humans-and-machines-for-better-crowdsourcing.aspx">Research Project Mixes Humans and Machines for Better Crowdsourcing</a></li>
<li>Neuraoscience News: <a href="http://neurosciencenews.com/human-machine-intelligence-framework-4221/">Crowd Augmented Cognition: Combining Human and Machine Intelligence to Accelerate Learning</a></li>
<li>DZone: <a href="https://dzone.com/articles/researchers-work-on-automated-means-of-managing-th">Research Suggests AI Managers Effective for Crowdsourcing</a></li>
<li>PhysOrg: <a href="https://phys.org/news/2016-05-crowd-augmented-cognition-team-tools-combine.html">Crowd-augmented cognition: Team develops tools that combine human and machine intelligence to accelerate learning</a></li>
<li>TechExplore: <a href="https://techxplore.com/news/2016-05-big-small-pieces-humans-crowdsourced.html">Big thinking in small pieces: Computer guides humans in crowdsourced research</a></li>
<li>Spend Matters: <a href="http://spendmatters.com/2016/06/09/crowdsourcing-and-cognitive-computing-are-you-ready-for-the-future-of-work/">Crowdsourcing and Cognitive Computing: Are You Ready for the Future of Work?</a></li>
<li>Science Daily: <a href="https://www.sciencedaily.com/releases/2016/05/160511210628.htm">Crowd-augmented cognition - combine human, machine intelligence to accelerate learning</a></li>
<li>EurekAlert: <a href="https://www.eurekalert.org/pub_releases/2016-05/cmu-bti051016.php">Big thinking in small pieces: Computer guides humans in crowdsourced research</a></li>
<li>EurekAlert: <a href="https://www.eurekalert.org/pub_releases/2016-05/nsf-cc051116.php">Crowd-augmented cognition</a></li>
<li>NSF News: <a href="https://www.nsf.gov/news/mmg/mmg_disp.jsp?med_id=80586&amp;from=">Big thinking in small pieces: Computer guides humans in crowdsourced research</a></li>
<li>NSF News: <a href="https://www.nsf.gov/news/news_summ.jsp?cntn_id=138580&amp;org=NSF">Crowd-augmented cognition</a></li>
<li>CMU SCS News: <a href="https://www.cmu.edu/news/stories/archives/2016/may/knowledge-accelerator.html">Big thinking in small pieces: Computer guides humans in crowdsourced research</a></li>
<li>CMU HCII News: <a href="https://hcii.cmu.edu/news/2016/hcii-chi-computer-guides-humans-crowdsourced-research">Computer Guides Humans in Crowdsourced Research</a></li>
</ul>


<h2>Citation</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Nathan Hahn, Joseph Chang, Ji Eun Kim, and Aniket Kittur. 2016.
</span><span class='line'>The Knowledge Accelerator: Big Picture Thinking in Small Pieces.
</span><span class='line'>In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI '16).
</span><span class='line'>ACM, New York, NY, USA, 2258-2270. DOI: http://dx.doi.org/10.1145/2858036.2858364</span></code></pre></td></tr></table></div></figure>


<h2>Bibtex</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>@inproceedings{Hahn:2016:KAB:2858036.2858364,
</span><span class='line'> author = {Hahn, Nathan and Chang, Joseph and Kim, Ji Eun and Kittur, Aniket},
</span><span class='line'> title = {The Knowledge Accelerator: Big Picture Thinking in Small Pieces},
</span><span class='line'> booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
</span><span class='line'> series = {CHI '16},
</span><span class='line'> year = {2016},
</span><span class='line'> isbn = {978-1-4503-3362-7},
</span><span class='line'> location = {Santa Clara, California, USA},
</span><span class='line'> pages = {2258--2270},
</span><span class='line'> numpages = {13},
</span><span class='line'> url = {http://doi.acm.org/10.1145/2858036.2858364},
</span><span class='line'> doi = {10.1145/2858036.2858364},
</span><span class='line'> acmid = {2858364},
</span><span class='line'> publisher = {ACM},
</span><span class='line'> address = {New York, NY, USA},
</span><span class='line'> keywords = {complex workflow, crowd work, crowdsourcing, design patterns, information synthesis},
</span><span class='line'>}
</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Twitter Code-Switching]]></title>
    <link href="http://josephcc.github.com/arXiv-twitter-code-switching/"/>
    <updated>2014-12-22T08:00:00-08:00</updated>
    <id>http://josephcc.github.com/arXiv-twitter-code-switching</id>
    <content type="html"><![CDATA[<p>Code-switching behavior is a common phenomenon on social media to express
solidarity or establish authority. While past work on automatic code-switching
detection depends on dictionary look-up or named-entity recognition, our
recurrent neural network model that relies on only raw features outperformed
the top systems in the EMNLP&#8217;14 Code-Switching Workshop by 17% in error rate
reduction.</p>

<p><span style='color: gray'>Final project for the Deep Learning course at CMU.</span></p>

<!--more-->


<h2>Abstract</h2>

<p>Mixed language data is one of the difficult yet less explored domains of
natural language processing. Most research in fields like machine translation
or sentiment analysis assume monolingual input. However, people who are capable
of using more than one language often communicate using multiple languages at
the same time. Sociolinguists believe this &#8220;code-switching&#8221; phenomenon to be
socially motivated. For example, to express solidarity or to establish
authority. Most past work depend on external tools or resources, such as
part-of-speech tagging, dictionary look-up, or named-entity recognizers to
extract rich features for training machine learning models. In this paper, we
train recurrent neural networks with only raw features, and use word embedding
to automatically learn meaningful representations. Using the same
mixed-language Twitter corpus, our system is able to outperform the best
SVM-based systems reported in the EMNLP&#8217;14 Code-Switching Workshop by 1% in
accuracy, or by 17% in error rate reduction.</p>

<h2>Download</h2>

<p><a class="btn btn-default" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Twitter', 'arXiv']);"  href="https://arxiv.org/abs/1412.4314" role="button">arXiv</a>
<a class="btn btn-default" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Twitter', 'PDF']);"  href="https://arxiv.org/pdf/1412.4314v2.pdf" role="button">arXiv hosted PDF</a></p>

<h2>Citation</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Chang, Joseph Chee, and Chu-Cheng Lin.
</span><span class='line'>"Recurrent-Neural-Network for Language Detection on Twitter Code-Switching Corpus."
</span><span class='line'>arXiv preprint arXiv:1412.4314 (2014).</span></code></pre></td></tr></table></div></figure>


<h2>Bibtex</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>@article{chang2014recurrent,
</span><span class='line'>  title={Recurrent-Neural-Network for Language Detection on Twitter Code-Switching Corpus},
</span><span class='line'>  author={Chang, Joseph Chee and Lin, Chu-Cheng},
</span><span class='line'>  journal={arXiv preprint arXiv:1412.4314},
</span><span class='line'>  year={2014}
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Hangman Game Solver based on Language Models]]></title>
    <link href="http://josephcc.github.com/hangman/"/>
    <updated>2012-11-04T21:14:00-08:00</updated>
    <id>http://josephcc.github.com/hangman</id>
    <content type="html"><![CDATA[<p>I&#8217;ve recently wrote a program for solving sentence-based hangman game automatically. Here&#8217;s a post
explaining my approach using the British National Corpus, <a href="http://www.speech.sri.com/projects/srilm/">srilm</a>
language model and hidden Markov model.</p>

<h2>The Hangman Game</h2>

<p>Last week, my friend <a href="http://itszero.github.com">Zero Cho</a> was asked to write a program that solves the
game of hangman automatically. The rules are
simple: the questions are short sentences or phrases with their characters
concealed, i.e. replaced with an underscore, while spaces and punctuations are revealed. The solving
process is an interactive process in which the solver program will guess a letter, and the question
system will reveal its locations. If you guessed 3 letters that are not in the sentence,
you fail the question.</p>

<p>For example, the initial hint for the answer <code>it's a sunny day</code> is <code>__'_ _ _____ ___</code>,
if a solver program guesses <code>y</code>, the system will respond with <code>__'_ _ ____y __y</code>.</p>

<!--more-->


<p>My friend Zero used a statistical approach that first guesses vows with highest frequencies until the first correct
guess. With one vow revealed, he then repeatedly choose the word with the most portion revealed to
continue guessing. His approach to guessing a half-revealed word is also statistical, namely to find the
most frequent English word that matches the pattern.  His method is explained
in detail in his recent <a href="http://itszero.github.com/blog/2012/10/29/my-way-to-write-a-hangman-ai/">blog post</a>.</p>

<p>The shortcoming of this approach is that it lacks consideration for correlations between words in the
sentences. For example, even the word <code>sorry</code> has a higher frequency than <code>sunny</code>, the latter is more
likely to appear before the word <code>day</code>. In my attempt, I trained a
<a href="http://en.wikipedia.org/wiki/Language_model">Language Model</a> to capture such correlations, and use
<a href="http://en.wikipedia.org/wiki/Hidden_Markov_model">hidden Markov model</a> to guess every word in the sentence
simultaneously.</p>

<h2>Language Model</h2>

<p>I use the state-of-the-art <a href="http://www.speech.sri.com/projects/srilm/">srilm</a> package to train my
language model on a portion of the British National Corpus (BNC). For those who are familiar with srilm,
the parameters I&#8217;ve used include order-5, Kneser-Ney discount, and interpolation. I use SWIG to
integrate srilm (in C) with my main program written in Python.</p>

<p>A language model is a statistical model that defines the probability of a sequence of words using
probability distribution trained on a free text corpus.</p>

<p>For example, given a sequence of words $w_1, w_2, w_3, &#8230;, w_n$, the unigram (one-word) language model
calculates the probability of such sequence as
$$P(w<em>{1}w</em>{2}w<em>{3}&#8230;w</em>{n}) := P(w_1) * P(w_2) * P(w_3) * &#8230; * P(w_n)$$
this is sometimes called an order-1 language model. The word probability can be calculated by counting
its appearances in the corpus:
$$ P(w) := \frac { count(w) } { |\ corpus\ | } $$
An order-2 language model, or bigram language model, defines the sequence probability using conditional
probability:
$$P(w<em>{1}w</em>{2}w<em>{3}&#8230;w</em>{n}) := P(w_1) * P(w_2|w_1) * P(w_3|w_2) * &#8230; * P(w_n|w<em>{n-1})$$
intuitively, conditional probability can be calculated as:
$$ P(w_a|w_b) := \frac { count(w</em>{b}w_{a}) } { count(w_b) } $$
Generally, higher order produces better results but also suffers more severely on the out-of-vocabulary problem
and therefore sometimes needs to fall back to lower order conditional probabilities. Many theories have been
proposed to improve the performance of language models, including absolute discount estimation, good turing,
Kneser-Ney smoothing. Formal definition of language model can be found on
<a href="http://en.wikipedia.org/wiki/Language_model">Wikipedia</a>.</p>

<h2>Hidden Markov Model</h2>

<p>Hidden Markov model is a statistical model that solves a sequence of hidden states given a sequence of
observations, state transition probability distribution, and emission probability distribution. For
example, in speech recognition, the recorded audio speech is the observation, the actual text are the
hidden states. Another example is the optical character recognition (OCR) where the scanned images are
the observations. Emission probability distribution indicates the probability from state to observation,
i.e., from text to audio or image. Language model is commonly used for state transition probability
distribution.</p>

<p>In the case solving the Hangman game, the hidden states are the English words in the answer sentences, e.g., <code>it's</code>, <code>a</code>,
<code>sunny</code>, <code>day</code>, and the observations are some half-revealed sequences, e.g., <code>__'s</code>, <code>_</code>, <code>s_nny</code>, <code>__y</code>.
The state transition probability is calculated by the trained language model, e.g., $P(day | sunny)$ in
a order-1 language model. I define the emission probability as the normalize probability of all the
English words that matches the half-revealed pattern, e.g., normalized $P(day), P(pay), P(bay), P(hey),
P(may), &#8230;, etc$. for state <code>__y</code>, and zero otherwise. With the transition and emission probabilities defined, we
can use the Viterbi dynamic programming algorithm to calculate the optimal state sequence.</p>

<p>Formal definition and more information on HMM can be found on
Wikipedia pages <a href="http://en.wikipedia.org/wiki/Hidden_Markov_model">here</a> and
<a href="http://en.wikipedia.org/wiki/Viterbi_algorithm">here</a>, or in a slide I made when I was TA for a NLP course
<a href="http://hadoop.nlpweb.org/~bizkit/lab3/vtb_example/">here</a> (in Chinese).</p>

<h2>Algorithm</h2>

<p>It is obvious that to run the hidden Markov model on a fully concealed sequence will yield bad results.
Therefore, at stage one my solver also start by guessing high frequency letters. Instead of
guessing vows only, I also use consonants, hoping that revealing some consonants will help the second
stage. The sorted list begins with <code>e, a, i, s, r, n, t, o</code>, &#8230;, etc. This
process stops until it had made one wrong guesses or have revealed more than four letters.</p>

<p>In the second stage, the trained hidden Markov model will tag the half-revealed words with English word
sequence that yield the highest language model probability and word frequencies. The solver will then
guess the letter that appears in most words. This process repeats until all characters are
revealed, and the number of failed guesses are recorded.</p>

<h2>Results</h2>

<p>Here are three examples and results. The interactive guessing progress and some internal states are
shown in the three tables below.</p>

<h3>&#8220;describe what is needed&#8221;</h3>

<p>stage   |errors |hint   | HMM prediction| guess
&#8212;&#8211;   |&#8212;&#8212;-    |&#8212;-   |&#8212;&#8212;-    | &#8212;&#8211;
1   |0  |<code>________ ____ __ ______</code>  | N/A   | e
1   |0  |<code>_e_____e ____ __ _ee_e_</code>  | N/A   | a
1   |0  |<code>_e_____e __a_ __ _ee_e_</code>  | N/A   | i
1   |0  |<code>_e___i_e __a_ i_ _ee_e_</code>  | N/A   | s
1   |0  |<code>_es__i_e __a_ is _ee_e_</code>  | N/A   | r
2   |0  |<code>_es_ri_e __a_ is _ee_e_</code>  | <strong>describe what is needed</strong>   | d
2   |0  |<code>des_ri_e __a_ is _eeded</code>  | <strong>describe what is needed</strong>   | n
2   |0  |<code>des_ri_e __a_ is needed</code>  | <strong>describe what is needed</strong>   | t
2   |0  |<code>des_ri_e __at is needed</code>  | <strong>describe what is needed</strong>   | t
2   |0  |<code>descri_e __at is needed</code>  | <strong>describe what is needed</strong>   | hbw (abrv.)
2   |0  |<code>describe what is needed</code>  | (solved)  |
<br/></p>

<h3>&#8220;an honor roll of online options&#8221;</h3>

<table>
<thead>
<tr>
<th>stage   </th>
<th>errors </th>
<th>hint   </th>
<th> HMM prediction</th>
<th> guesses</th>
</tr>
</thead>
<tbody>
<tr>
<td>1   </td>
<td>0  </td>
<td><code>__ _____ ____ __ ______ _______</code>  </td>
<td>N/A    </td>
<td> eaisr</td>
</tr>
<tr>
<td>2   </td>
<td>0  </td>
<td><code>a_ ____r r___ __ ___i_e ___i__s</code>  </td>
<td>at honor roll of police options    </td>
<td> o</td>
</tr>
<tr>
<td>2   </td>
<td>0  </td>
<td><code>a_ _o_or ro__ o_ o__i_e o__io_s</code>  </td>
<td>at honor roll of office options    </td>
<td> n</td>
</tr>
<tr>
<td>2   </td>
<td>0  </td>
<td><code>an _onor ro__ o_ on_ine o__ions</code>  </td>
<td><strong>an honor roll of online options</strong>    </td>
<td> l</td>
</tr>
<tr>
<td>2   </td>
<td>0  </td>
<td><code>an _onor roll o_ online o__ions</code>  </td>
<td><strong>an honor roll of online options</strong>    </td>
<td> t</td>
</tr>
<tr>
<td>2   </td>
<td>0  </td>
<td><code>an _onor roll o_ online o_tions</code>  </td>
<td><strong>an honor roll of online options</strong>    </td>
<td> hpf</td>
</tr>
<tr>
<td>2   </td>
<td>0  </td>
<td><code>an honor roll of online options</code>  </td>
<td>(solved)   </td>
<td></td>
</tr>
</tbody>
</table>


<p>This one shows HMM produces different results as more characters are revealed.
<br/></p>

<h3>&#8220;members of the supreme court&#8221;</h3>

<table>
<thead>
<tr>
<th>stage   </th>
<th>errors </th>
<th>hint   </th>
<th> HMM prediction</th>
<th> guesses</th>
</tr>
</thead>
<tbody>
<tr>
<td>1   </td>
<td>0  </td>
<td><code>_______ __ ___ _______ _____</code> </td>
<td> N/A   </td>
<td> e</td>
</tr>
<tr>
<td>1   </td>
<td>0  </td>
<td><code>_e__e__ __ __e ____e_e _____</code> </td>
<td> N/A   </td>
<td> a</td>
</tr>
<tr>
<td>2   </td>
<td>1  </td>
<td><code>_e__e__ __ __e ____e_e _____</code> </td>
<td> <strong>members of the supreme court</strong>  </td>
<td> rstoumchpbf</td>
</tr>
<tr>
<td>2   </td>
<td>1  </td>
<td><code>members of the supreme court</code> </td>
<td>   </td>
<td></td>
</tr>
</tbody>
</table>


<p>This last one really shows the power of language model. With only <code>e</code> revealed, the HMM process was able to guess the correct result.</p>

<h2>Improvements</h2>

<p>For stage one, instead of using just the highest frequency letters, conditional probability can also be
a factor for choosing the next letter. For example, if <code>s</code> is revealed as the second to last character
in some words, <code>t</code> and <code>e</code> may likely to be the last letter of such words.</p>

<p>For the two stages, we can decide to advance from stage 1 to 2 base on different factors instead of
heuristically. These may include the confident of the HMM results, the percentage of letters revealed,
the before mentioned conditional probabilities and more. Global optimization approaches can be used to
find good parameters to determine the timing to start stage two.  Alternatively, we can treat the two
stages as different strategies and use genetic algorithm to apply them alternately.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[TermMine]]></title>
    <link href="http://josephcc.github.com/ACL-termmine/"/>
    <updated>2012-07-08T08:00:00-07:00</updated>
    <id>http://josephcc.github.com/ACL-termmine</id>
    <content type="html"><![CDATA[<p>TermMine is an information extraction system that can automatically mine
translation pairs of terms from the web. We used a small set of terms and
translations to gather mixed-code text from the web to train a CRF model that
can identify translation pairs at run-time.</p>

<!--more-->


<h2>Abstract</h2>

<p>In this paper, we present a new method for learning to finding translations and
transliterations on the Web for a given term. The approach involves using a
small set of terms and translations to obtain mixed-code snippets from a search
engine, and automatically annotating the snippets with tags and features for
training a conditional random field model. At run-time, the model is used to
extracting translation candidates for a given term. Preliminary experiments and
evaluation show our method cleanly combining various features, resulting in a
system that outperforms previous work.</p>

<h2>Download</h2>

<p><a class="btn btn-default" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'TermMine', 'PDF']);"  href="http://www.aclweb.org/anthology/P12-2026" role="button">ACLWeb Hosted PDF</a>
<a class="btn btn-default" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'TermMine', 'ACM']);"  href="http://dl.acm.org/citation.cfm?id=2390665.2390698" role="button">ACM Digital Library</a></p>

<h2>Citation</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Joseph Z. Chang, Jason S. Chang, and Jyh-Shing Roger Jang. 2012.
</span><span class='line'>Learning to find translations and transliterations on the web.
</span><span class='line'>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics.
</span><span class='line'>Association for Computational Linguistics, Stroudsburg, PA, USA, 130-134.</span></code></pre></td></tr></table></div></figure>


<h2>Bibtex</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>@inproceedings{Chang:2012:LFT:2390665.2390698,
</span><span class='line'> author = {Chang, Joseph Z. and Chang, Jason S. and Jang, Jyh-Shing Roger},
</span><span class='line'> title = {Learning to Find Translations and Transliterations on the Web},
</span><span class='line'> booktitle = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics},
</span><span class='line'> series = {ACL '12},
</span><span class='line'> year = {2012},
</span><span class='line'> location = {Jeju Island, Korea},
</span><span class='line'> pages = {130--134},
</span><span class='line'> numpages = {5},
</span><span class='line'> url = {http://dl.acm.org/citation.cfm?id=2390665.2390698},
</span><span class='line'> acmid = {2390698},
</span><span class='line'> publisher = {Association for Computational Linguistics},
</span><span class='line'> address = {Stroudsburg, PA, USA},
</span><span class='line'>} </span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Git: Remove (un-add) files from last commit]]></title>
    <link href="http://josephcc.github.com/git-reverse-amend/"/>
    <updated>2012-04-27T03:07:00-07:00</updated>
    <id>http://josephcc.github.com/git-reverse-amend</id>
    <content type="html"><![CDATA[<p>A lot of times we use <code>git commit --amend</code> to add the files that we have forgotten to the last
commit (usually HEAD), or simply to fix typos in its commit message. However, you can&#8217;t remove, or un-add,
committed files from the latest commit with <code>git commit --amend</code>. Here&#8217;s how:</p>

<!-- more -->


<ul>
<li>Point your HEAD to the commit prior to the latest commit, without touching the index nor the working
tree:</li>
</ul>


<figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span></span>git reset --soft HEAD
</span></code></pre></td></tr></table></div></figure>


<ul>
<li>Now your <code>HEAD</code> points to the commit prior to the lastest commit, your <code>ORIG_HEAD</code> points to the latest
commit, and your working tree and staging area is the same as the latest commit. Now we unstage all the
changes from the staging area (or index), and add the files you originally intended to commit:</li>
</ul>


<figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span></span>git reset HEAD .
</span><span class='line'>git add &lt;files&gt;
</span></code></pre></td></tr></table></div></figure>


<ul>
<li>Finally, commit the staged files and reuse the metadata of the wrong commit:</li>
</ul>


<figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span></span>git commit -c ORIG_HEAD
</span></code></pre></td></tr></table></div></figure>


<p> you can always you <code>git rebase</code> to turn back time.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Linux Kernel: Module debugging with gdb]]></title>
    <link href="http://josephcc.github.com/linux-kernel-dev-module-debugging-gdb-kdgb/"/>
    <updated>2011-12-05T21:12:00-08:00</updated>
    <id>http://josephcc.github.com/linux-kernel-dev-module-debugging-gdb-kdgb</id>
    <content type="html"><![CDATA[<p>This is post continues my previous post on
<a href="http://joe.cat/blog/2011/10/17/linux-kernel-dev-debug-qemu-kgdb/">Kernel Debugging with gdb</a>,
and explain how to do the same for debugging kernel modules.</p>

<!-- more -->


<p>此篇文章延續前文 ，簡單介紹如何使用 gdb / kgdb 對核心模組 (kernel module)
進行即時的 debugging。</p>

<p>The previous post explained how to use QEMU, kgdb and gdb to debug the Linux
kernel in realtime. Adding parameters to <code>.config</code>, debug information is
included in <code>vmlinux</code>, so that in gdb you can press <code>l</code> to list currently
running kernel codes. However, kernel modules are dynamically loaded, even with
the <code>-g</code> compile flag, the debug information is not loaded with <code>vmlinux</code>
into gdb.</p>

<p>在先前的文章中，介紹了如何使用 qemu、kgdb、gdb 對核心進行動態的
debugging，並在編譯核心時在 vmlinux 中包含 debug information，使 debug
時能夠直接瀏覽原始碼以及對變數進行存取。然而，kernel module
是在核心之外的，因次就算編譯時加上 -g GCCFLAG 也無法直接在 gdb 中存取其
debug information。</p>

<p>To solve this issue, we need to use the <code>add-symbol-file</code> to let gdb read
additional debug information. Since in my setup, gdb and the dynamically
loaded kernel module are executing on different machines, there&#8217;s no way for gdb
to know kernel module&#8217;s location in the memory after executing <code>insmod</code>. Here&#8217;s
my solution:</p>

<p>要解決這個問題，需要使用 gdb 中的 add-symbol-file 命令來讀入包含 debug
information 的 kernel module。gdb 與核心、模組是執行在完全不同的電腦上，因次
gdb 是無法直接得知 insmod 之後 kernel module 被載入到記憶體中的位置，也讓
module debugging 變得比較 tricky 一點。以下是我的作法：</p>

<ol>
<li>In the kernel module, add a break point stub in <code>init_module</code>, so that the
kernel is immediately stopped when the module is loaded for us to add more break
points.</li>
<li>When compiling the module, use <code>-g -O0</code> to produce debug information and
prevent optimization.</li>
<li>With the VM connected to gdb, use <code>insmod</code> to load the module. The VM will
stop at <code>init_module</code>, use <code>c</code> in gdb to continue running the kernel for now.</li>
<li>You can now find the module&#8217;s location in memory in the file
<code>/sys/modules/&lt;module_name&gt;/sections/.text</code>. Copy it, and use other break points
to stop the kernel without reloading the module.</li>
<li>In gdb, execute <code>add-symbol-file &lt;module&gt; &lt;location&gt;</code> to load the debug
information.</li>
</ol>


<p><a href="http://www.flickr.com/photos/bizkit/5233432557/" title="Screen shot
2010-12-05 at 4.55.43 PM by bizkit@tw, on Flickr"><img
src="http://farm6.staticflickr.com/5163/5233432557_a05d118698_z.jpg" width="640"
height="266" alt="Screen shot 2010-12-05 at 4.55.43 PM"></a></p>

<p>Now you can list and step through module code in gdb.</p>

<p>Ref: http://www.linuxjournal.com/article/4525?page=0,1</p>
]]></content>
  </entry>
  
</feed>
