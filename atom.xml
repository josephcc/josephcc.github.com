<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Joseph Chee Chang]]></title>
  <link href="http://josephcc.github.com/atom.xml" rel="self"/>
  <link href="http://josephcc.github.com/"/>
  <updated>2023-04-12T18:09:11-07:00</updated>
  <id>http://josephcc.github.com/</id>
  <author>
    <name><![CDATA[Joseph Chee Chang]]></name>
    <email><![CDATA[josephc->allenai*org]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[CiteSee]]></title>
    <link href="http://josephcc.github.com/CHI-citesee/"/>
    <updated>2023-04-23T08:10:00-07:00</updated>
    <id>http://josephcc.github.com/CHI-citesee</id>
    <content type="html"><![CDATA[<p><strong><em><i class="fa fa-trophy" aria-hidden="true"></i> BEST PAPER AWARD</em></strong>
<br/></p>

<p> CiteSee provides a personalized paper reading experience by visually
 augmenting inline citations based on their connections to the current user
 based on their reading history, paper library, and publication records.
 During literature review sessions, CiteSee allows users to prioritize their
 exploration to focus first on inline citations most relevant to their recent
 readings. It also highlights inline citations to familiar papers, allowing
 users to keep track of their exploration and better contexturalize the current
 paper. Lab and field studies showed CiteSee can improve better paper discovery
 via inline citations, and allowed participants to have better situational
 awareness when conducting real-world literature reviews.</p>

<!--more-->


<h2>Abstract</h2>

<p>When reading a scholarly article, inline citations help researchers
contextualize the current article and discover relevant prior work. However, it
can be challenging to prioritize and make sense of the hundreds of citations
encountered during literature reviews. This paper introduces CiteSee, a paper
reading tool that leverages a user&#8217;s publishing, reading, and saving activities
to provide personalized visual augmentations and context around citations.
First, CiteSee connects the current paper to familiar contexts by surfacing
known citations a user had cited or opened. Second, CiteSee helps users
prioritize their exploration by highlighting relevant but unknown citations
based on saving and reading history. We conducted a lab study that suggests
CiteSee is significantly more effective for paper discovery than three
baselines. A field deployment study shows CiteSee helps participants keep
track of their explorations and leads to better situational awareness and
increased paper discovery via inline citation when conducting real-world
literature reviews.</p>

<h2>Presentation (SIGCHI 2023)</h2>

<p>coming soon&#8230;</p>

<h2>Downloads</h2>

<p><a class="btn btn-default" href="https://arxiv.org/abs/2302.07302" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'CiteSee', 'arXiv']);"  role="button">arXiv</a></p>

<h2>Citation</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Joseph Chee Chang, Amy X. Zhang, Jonathan Bragg, Andrew Head, Kyle
</span><span class='line'>Lo, Doug Downey, and Daniel S. Weld. 2023.
</span><span class='line'>CiteSee: Augmenting Citations in Scientific Papers with Persistent and Personalized Historical Context.
</span><span class='line'>In Proceedings of the 2023 CHI Conference on Human Factors in Computing
</span><span class='line'>Systems (CHI ’23), April 23–28, 2023, Hamburg, Germany. ACM, New York,
</span><span class='line'>NY, USA, Article 111, 15 pages. https://doi.org/10.1145/3544548.3580847</span></code></pre></td></tr></table></div></figure>


<h2>Bibtex</h2>

<p>coming soon</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Relatedly]]></title>
    <link href="http://josephcc.github.com/CHI-relatedly/"/>
    <updated>2023-04-23T08:05:00-07:00</updated>
    <id>http://josephcc.github.com/CHI-relatedly</id>
    <content type="html"><![CDATA[<p>Scientific breakthroughs often rely upon scholars synthesizing many published
work into broad overviews and rich insights to identify gaps in the current
literature.  However, survey articles require significant time and effort to
synthesize and can quickly become outdated. Researchers in fast-paced
disciplines also rely on the related work section to better understand the
background when reading a paper.  While related work sections also summarizes
multiple prior work, they typically provide partial views of the larger
research domain. Relatedly explore how a system in which users can quickly
explore and read related work sections extracted across many papers can help
scholars gain richer and more comprehensive overviews of fast-paced domains.</p>

<!--more-->


<h2>Abstract</h2>

<p>Scholars who want to research a scientific topic must take time to read,
extract meaning, and identify connections across many papers. As scientific
literature grows, this becomes increasingly challenging. Meanwhile, authors
summarize prior research in papers&#8217; related work sections, though this is
scoped to support a single paper. A formative study found that while reading
multiple related work paragraphs helps overview a topic, it is hard to navigate
overlapping and diverging references and research foci.  In this work, we
design a system, Relatedly, that scaffolds exploring and reading multiple
related work paragraphs on a topic, with features including dynamic re-ranking
and highlighting to spotlight unexplored dissimilar information, auto-generated
descriptive paragraph headings, and low-lighting of redundant information. From
a within-subjects user study (n=15), we found that scholars generate more
coherent, insightful, and comprehensive topic outlines using Relatedly compared
to a baseline paper list.</p>

<h2>Presentation (SIGCHI 2023)</h2>

<p>coming soon&#8230;</p>

<h2>Downloads</h2>

<p><a class="btn btn-default" href="https://arxiv.org/abs/2302.06754" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Relatedly', 'arXiv']);"  role="button">arXiv</a></p>

<h2>Citation</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Srishti Palani, Aakanksha Naik, Doug Downey, Amy X. Zhang, Jonathan
</span><span class='line'>Bragg, and Joseph Chee Chang. 2023. Relatedly: Scaffolding Literature
</span><span class='line'>Reviews with Existing Related Work Sections. In Proceedings of the 2023
</span><span class='line'>CHI Conference on Human Factors in Computing Systems (CHI ’23), April
</span><span class='line'>23–28, 2023, Hamburg, Germany. ACM, New York, NY, USA, 19 pages.
</span><span class='line'>https://doi.org/10.1145/3544548.3580841</span></code></pre></td></tr></table></div></figure>


<h2>Bibtex</h2>

<p>coming soon</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ComLittee]]></title>
    <link href="http://josephcc.github.com/CHI-comlittee/"/>
    <updated>2023-04-23T08:00:00-07:00</updated>
    <id>http://josephcc.github.com/CHI-comlittee</id>
    <content type="html"><![CDATA[<p>Significant research has been devoted to creating systems that help scholars
discover relevant papers.  While recent approaches have also shown benefits in
highlighting relevant authors to improve paper discovery, these systems do not
capture and utilize users&#8217; evolving knowledge of authors.  We introduce
ComLittee, a literature discovery system that supports author-centric
exploration.  ComLittee supports author discovery to curation of a
comittee of authors relevant to a research topic of interests based on existing
paper recommendation systems.  In our evaluation, we demonstrate how ComLittee
leads to a higher efficiency, quality, and novelty in author discovery that
also improves paper discovery.</p>

<!--more-->


<h2>Abstract</h2>

<p>In order to help scholars understand and follow a research topic, significant
research has been devoted to creating systems that help scholars discover
relevant papers and authors.  Recent approaches have shown the usefulness of
highlighting relevant authors while scholars engage in paper discovery.
However, these systems do not capture and utilize users&#8217; evolving knowledge of
authors.  We reflect on the design space and introduce ComLittee, a literature
discovery system that supports author-centric exploration.  In contrast to
paper-centric interaction in prior systems, ComLittee&#8217;s author-centric
interaction supports curation of research threads from individual authors,
finding new authors and papers with combined signals from a paper recommender
and the curated authors&#8217; authorship graphs, and understanding them in the
context of those signals.  In a within-subjects experiment that compares to an
author-highlighting approach, we demonstrate how ComLittee leads to a higher
efficiency, quality, and novelty in author discovery that also improves paper
discovery.</p>

<h2>Presentation (SIGCHI 2023)</h2>

<p>coming soon&#8230;</p>

<h2>Downloads</h2>

<p><a class="btn btn-default" href="https://arxiv.org/abs/2302.06780" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'ComLittee', 'arXiv']);"  role="button">arXiv</a></p>

<h2>Citation</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Hyeonsu B. Kang, Nouran Soliman, Matt Latzke, Joseph Chee Chang, and Jonathan
</span><span class='line'>Bragg. 2023. ComLittee: Literature Discovery with Personal Elected Author
</span><span class='line'>Committees. In Proceedings of the 2023 CHI Conference on Human Factors in
</span><span class='line'>Computing Systems (CHI ’23), April 23–28, 2023, Hamburg, Germany. ACM,
</span><span class='line'>New York, NY, USA, 20 pages. https://doi.org/10.1145/3544548.3581371</span></code></pre></td></tr></table></div></figure>


<h2>Bibtex</h2>

<p>coming soon</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Threddy]]></title>
    <link href="http://josephcc.github.com/UIST-Threddy/"/>
    <updated>2022-10-29T07:00:00-07:00</updated>
    <id>http://josephcc.github.com/UIST-Threddy</id>
    <content type="html"><![CDATA[<p>Reviewing the literature to understand relevant past work is a critical part of
research. However, as the scientific literature grows the challenges for users
to find and make sense of the many different threads of research grow as well.
In this work we explore a tool integrated into users&#8217; reading process that
helps them with leveraging authors&#8217; existing summarization of prior research
threads such as in related work sections.  We developed Threddy that supports
efficient extraction and organization of threads along with supporting evidence
as scientists read research articles. The system then recommends further
relevant articles based on user-created threads.  Our lab study showed that
Threddy helps scientists to follow and curate research threads without breaking
out of their flow of reading, collect relevant papers and clips, and discover
interesting new articles to further grow threads.</p>

<!--more-->


<h2>Abstract</h2>

<p>Reviewing the literature to understand relevant threads of past work is a
critical part of research and vehicle for learning. However, as the scientific
literature grows the challenges for users to find and make sense of the many
different threads of research grow as well. Previous work has helped scholars
to find and group papers with citation information or textual similarity using
standalone tools or overview visualizations. Instead, in this work we explore a
tool integrated into users&#8217; reading process that helps them with leveraging
authors&#8217; existing summarization of threads, typically in introduction or
related work sections, in order to situate their own work&#8217;s contributions. To
explore this we developed a prototype that supports efficient extraction and
organization of threads along with supporting evidence as scientists read
research articles. The system then recommends further relevant articles based
on user-created threads. We evaluate the system in a lab study and find that it
helps scientists to follow and curate research threads without breaking out of
their flow of reading, collect relevant papers and clips, and discover
interesting new articles to further grow threads.</p>

<h2>Downloads</h2>

<p><a class="btn btn-default" href="http://josephcc.github.com/images/papers/threddy.pdf" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Threddy', 'PDF']);" role="button">PDF Download</a>
<a class="btn btn-default" href="https://arxiv.org/abs/2208.03455" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Threddy', 'arXiv']);"  role="button">arXiv</a></p>

<h2>Citation</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Hyeonsu B. Kang, Joseph Chee Chang, Yongsung Kim, Aniket Kittur
</span><span class='line'>"Threddy: An Interactive System for Personalized Thread-based Exploration and Organization of Scientific Literature"
</span><span class='line'>In Proceedings of the 35th ACM User Interface Software and Technology Symposium: UIST 2022.</span></code></pre></td></tr></table></div></figure>


<h2>Bibtex</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>@article{kang2022threddy,
</span><span class='line'> title={Threddy: An Interactive System for Personalized Thread-based Exploration and Organization of Scientific Literature},
</span><span class='line'> author={Kang, Hyeonsu B and Chang, Joseph Chee and Kim, Yongsung and Kittur, Aniket},
</span><span class='line'> booktitle = {},
</span><span class='line'> series = {UIST '22},
</span><span class='line'> year = {2022},
</span><span class='line'> location = {Bend, OR, USA},
</span><span class='line'> publisher = {ACM},
</span><span class='line'> address = {New York, NY, USA}
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Fuse]]></title>
    <link href="http://josephcc.github.com/UIST-Fuse/"/>
    <updated>2022-10-29T06:00:00-07:00</updated>
    <id>http://josephcc.github.com/UIST-Fuse</id>
    <content type="html"><![CDATA[<p>While people can fluidly collect, make sense of  and organize information
across many online sources in their minds to make informed decisions, the
amount of online information can be overwhelming and exceed people&#8217;s working
memory.  We introduce Fuse, a browser extension that combined low-cost
collection with lightweight organization of web content in a compact card-based
sidebar.  Fuse helps users simultaneously extract key web content and structure
it in a lightweight and visual way.  A 22-month public deployment and
interviews provide longitudinal insights into the collecting, externalization
and  structuring behaviors of real-world users conducting information foraging
tasks.</p>

<!--more-->


<h2>Abstract</h2>

<p>People spend a significant amount of time trying to make sense of the internet,
collecting content from a variety of sources and organizing it to make
decisions and achieve their goals. While humans are able to fluidly iterate on
collecting and organizing information in their minds, existing tools and
approaches introduce significant friction into the process. We introduce Fuse,
a browser extension that externalizes users&#8217; working memory by combining
low-cost collection with lightweight organization of content in a compact
card-based sidebar that is always available. Fuse helps users simultaneously
extract key web content and structure it in a lightweight and visual way. We
discuss how these affordances help users externalize more of their mental model
into the system (e.g., saving, annotating, and structuring items) and support
fast reviewing and resumption of task contexts. Our 22-month public deployment
and follow-up interviews provide longitudinal insights into the structuring
behaviors of real-world users conducting information foraging tasks.</p>

<h2>Downloads</h2>

<p><a class="btn btn-default" href="http://josephcc.github.com/images/papers/fuse.pdf" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Fuse', 'PDF']);" role="button">PDF Download</a>
<a class="btn btn-default" href="https://arxiv.org/abs/2208.14861" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Fuse', 'arXiv']);"  role="button">arXiv</a></p>

<h2>Citation</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Kuznetsov, Andrew, Joseph Chee Chang, Nathan Hahn, Napol Rachatasumrit, Bradley Breneisen, Julina Coupland, and Aniket Kittur.
</span><span class='line'>"Fuse: In-Situ Sensemaking Support in the Browser."
</span><span class='line'>In Proceedings of the 35th ACM User Interface Software and Technology Symposium: UIST 2022.</span></code></pre></td></tr></table></div></figure>


<h2>Bibtex</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>@article{kuznetsov2022fuse,
</span><span class='line'> title={Fuse: In-Situ Sensemaking Support in the Browser},
</span><span class='line'> author={Kuznetsov, Andrew and Chang, Joseph Chee and Hahn, Nathan and Rachatasumrit, Napol and Breneisen, Bradley and Coupland, Julina and Kittur, Aniket},
</span><span class='line'> booktitle = {},
</span><span class='line'> series = {UIST '22},
</span><span class='line'> year = {2022},
</span><span class='line'> location = {Bend, OR, USA},
</span><span class='line'> publisher = {ACM},
</span><span class='line'> address = {New York, NY, USA}
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Wigglite]]></title>
    <link href="http://josephcc.github.com/UIST-Wigglite/"/>
    <updated>2022-10-29T05:00:00-07:00</updated>
    <id>http://josephcc.github.com/UIST-Wigglite</id>
    <content type="html"><![CDATA[<p>Users conducting online sensemaking tasks face the challenge of capturing the
information they find for later use without interrupting their flow.
Specifically, as people collect information, they also need to triaging support
such as how urgent a topic is to follow up on, or rating a piece of evidence as
a &#8220;pro&#8221; or &#8220;con,&#8221; which helps scaffold subsequent deeper exploration.  However,
current approaches incur a high cost, often requiring users to select, copy,
context switch, paste, and annotate information.  In this work, we explore a
new interaction technique called &#8220;wiggling,&#8221; which can be used to fluidly
collect, organize, and rate information during early sensemaking stages with a
single gesture. Through implementation and user evaluation, we found that
wiggling helped participants accurately collect information and encode their
mental context with a 58% reduction in operational cost while being 24% faster
compared to a common baseline.</p>

<!--more-->


<h2>Abstract</h2>

<p>Consumers conducting comparison shopping, researchers making sense of
competitive space, and developers looking for code snippets online all face the
challenge of capturing the information they find for later use without
interrupting their current flow. In addition, during many learning and
exploration tasks, people need to externalize their mental context, such as
estimating how urgent a topic is to follow up on, or rating a piece of evidence
as a &#8220;pro&#8221; or &#8220;con,&#8221; which helps scaffold subsequent deeper exploration.
However, current approaches incur a high cost, often requiring users to select,
copy, context switch, paste, and annotate information in a separate document
without offering specific affordances that capture their mental context. In
this work, we explore a new interaction technique called &#8220;wiggling,&#8221; which can
be used to fluidly collect, organize, and rate information during early
sensemaking stages with a single gesture. Wiggling involves rapid
back-and-forth movements of a pointer or up-and-down scrolling on a smartphone,
which can indicate the information to be collected and its valence, using a
single, light-weight gesture that does not interfere with other interactions
that are already available. Through implementation and user evaluation, we
found that wiggling helped participants accurately collect information and
encode their mental context with a 58% reduction in operational cost while
being 24% faster compared to a common baseline.</p>

<h2>Downloads</h2>

<p><a class="btn btn-default" href="http://josephcc.github.com/images/papers/wigglite.pdf" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Wigglite', 'PDF']);" role="button">PDF Download</a>
<a class="btn btn-default" href="https://arxiv.org/abs/2208.00496" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Wigglite', 'arXiv']);"  role="button">arXiv</a></p>

<h2>Citation</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Michael Xieyang Liu, Andrew Kuznetsov, Yongsung Kim, Joseph Chee Chang, Aniket Kittur, Brad A Myers
</span><span class='line'>"Wigglite: Low-cost Information Collection and Triage"
</span><span class='line'>In Proceedings of the 35th ACM User Interface Software and Technology Symposium: UIST 2022.</span></code></pre></td></tr></table></div></figure>


<h2>Bibtex</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>@article{liu2022wigglite,
</span><span class='line'> title={Wigglite: Low-cost Information Collection and Triage},
</span><span class='line'> author={Liu, Michael Xieyang and Kuznetsov, Andrew and Kim, Yongsung and Chang, Joseph Chee and Kittur, Aniket and Myers, Brad A.},
</span><span class='line'> booktitle = {},
</span><span class='line'> series = {UIST '22},
</span><span class='line'> year = {2022},
</span><span class='line'> location = {Bend, OR, USA},
</span><span class='line'> publisher = {ACM},
</span><span class='line'> address = {New York, NY, USA}
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tabs.do]]></title>
    <link href="http://josephcc.github.com/UIST-tabsdo/"/>
    <updated>2021-10-10T08:00:00-07:00</updated>
    <id>http://josephcc.github.com/UIST-tabsdo</id>
    <content type="html"><![CDATA[<p>The nature of people’s online activities has gone through dramatic changes in
past decades, yet browser interfaces have stayed largely the same since tabs
were introduced nearly 20 years ago. The divide between browser interfaces that
only provide affordances for managing individual tabs and users who manage
their attention at the task level can lead to an &#8220;tab overload&#8221; problem. We
explored a task-centric tab manager called which enables users to save their
open tabs to manage them with task structures and affordances that better
reflect their mental models. To lower the cost of importing, Tabs.do uses a
neural network in the browser to make suggestions for grouping users&#8217; open tabs
by tasks.</p>

<!--more-->


<h2>Abstract</h2>

<p>The nature of people’s online activities has gone through dramatic changes in
past decades as significant portions of our productivity and sensemaking tasks
continue to migrate to “the cloud,” yet browser interfaces have stayed
largely the same since tabs were introduced nearly 20 years ago. The divide
between browser interfaces that only provide affordances for managing
individual tabs and users who manage their attention at the task level can lead
to serious adverse effects &#8211; commonly referred to as “tab overload.” This
paper explores the design of a task-centric tab manager called Tabs.do, which
enables users to import and close their open tabs to manage them as tasks.
Users can structure and prioritize their tasks to better reflect their mental
models and resume progress by reopening tasks into open tabs. To lower the cost
of importing, Tabs.do uses machine learning to make intelligent suggestions for
grouping users’ open tabs into task bundles with high precision by exploiting
behavioral and semantic features. Tabs.do bridges the gap between current
browser designs that treat tabs as stacks of independent webpages and users who
manage their workflow and attention at the tasks level. We conducted a field
deployment study where participants used Tabs.do with their real-life tasks and
tabs in the wild and uncovered insights around the costs, benefits, and
limitations of a task-centric approach to tab management.</p>

<h2>30 Seconds Preview (UIST 2021)</h2>

<iframe width="560" height="315" src="https://www.youtube.com/embed/he--Ly0UQ-4" frameborder="0" allowfullscreen></iframe>


<h2>Presentation (UIST 2021)</h2>

<iframe width="560" height="315" src="https://www.youtube.com/embed/ZwbVzDRFbGs" frameborder="0" allowfullscreen></iframe>


<h2>Downloads</h2>

<p><a class="btn btn-default" href="http://josephcc.github.com/images/papers/tabs.do.pdf" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'TabsDo', 'PDF']);" role="button">PDF Download</a>
<a class="btn btn-default" href="https://dl.acm.org/doi/10.1145/3472749.3474777" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'TabsDo', 'ACM']);"  role="button">ACM Digital Library</a></p>

<h2>Media Coverage</h2>

<ul>
<li>Mashable: <a href="https://mashable.com/article/too-many-tabs-open">Stop trying to work in multiple browser tabs. It&#8217;s terrible for your focus.</a></li>
<li>Fast Company: <a href="https://www.fastcompany.com/90635776/the-twisted-psychology-of-browser-tabs-and-why-we-cant-get-rid-of-them">The twisted psychology of browser tabs—and why we can&#8217;t get rid of them</a></li>
<li>Make Use Of: <a href="https://www.makeuseof.com/chrome-extensions-manage-too-many-tabs/">5 More Chrome Extensions to Manage the &#8220;Too Many Tabs&#8221; Problem</a></li>
<li>MetroNews UK: <a href="https://metro.co.uk/2021/05/10/suffer-from-tab-overload-scientists-study-why-we-have-so-many-open-14540577/amp/">Suffer from &#8220;tab overload&#8221;? Scientists study why we have so many open</a></li>
<li>Inc.: <a href="https://www.inc.com/jessica-stillman/productivity-browser-tabs-carnegie-mellon.html">Stressed Out by Your 87 Open Browser Tabs? New Science Offers a Fix</a></li>
<li>HackerNews: <a href="https://news.ycombinator.com/item?id=27095701">When the Tab Comes Due: Challenges in the Cost Structure of Browser Tab Usage</a></li>
<li>HackerNews: <a href="https://news.ycombinator.com/item?id=27157225">Overcoming tab overload</a></li>
<li>Science Alert: <a href="https://www.sciencealert.com/tab-overload-is-a-common-problem-for-people-browsing-the-internet-survey-finds">We&#8217;re Getting Buried in Browser Tabs And Scientists Want to Fix It</a></li>
<li>EurekAlert: <a href="https://www.eurekalert.org/pub_releases/2021-05/cmu-oto050721.php">Overcoming tab overload</a></li>
<li>NewsGram: <a href="https://www.newsgram.com/skeema-this-tool-will-assist-you-in-managing-your-browser-tabs-more-effectively/">Skeema: This Tool Will Assist You In Managing Your Browser Tabs More Effectively</a></li>
<li>Sify: <a href="https://www.sify.com/news/this-tool-can-help-you-better-manage-browser-tabs-news-education-vfjl5Ebfaiifc.html">This tool can help you better manage browser tabs</a></li>
<li>Revyuh: <a href="https://www.revyuh.com/news/software/apps/browser-tabs-scientists-find-new-way-to-overcome-fear-of-black-hole-effect/">Browser Tabs: Scientists Find New Way to Overcome Fear of Black Hole Effect</a></li>
<li>Carnegie Mellon University: <a href="https://www.cmu.edu/news/stories/archives/2021/may/overcoming-tab-overload.html">Overcoming Tab Overload</a></li>
</ul>


<h2>Citation</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Joseph Chee Chang, Yongsung Kim, Victor Miller, Michael Xieyang Liu, Brad Myers, Aniket Kittur.
</span><span class='line'>Tabs.do: Task-Centric Browser Tab Management.
</span><span class='line'>In Proceedings of the 34th ACM User Interface Software and Technology Symposium: UIST 2021.</span></code></pre></td></tr></table></div></figure>


<h2>Bibtex</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>@article{chang2021tabsdo,
</span><span class='line'>  title={Tabs.do: Task-Centric Browser Tab Management.},
</span><span class='line'>  author={Chang, Joseph Chee and Kim, Yongsung and Miller, Victor and Liu, Michael Xieyang and Myers, Brad and Kittur, Aniket},
</span><span class='line'>  booktitle = {Proceedings of the 34th ACM User Interface Software and Technology Symposium},
</span><span class='line'>  series = {UIST'21},
</span><span class='line'>  year={2021}
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[When the Tab Comes Due]]></title>
    <link href="http://josephcc.github.com/CHI-browser-tabs/"/>
    <updated>2021-05-08T08:00:00-07:00</updated>
    <id>http://josephcc.github.com/CHI-browser-tabs</id>
    <content type="html"><![CDATA[<p><strong><em><i class="fa fa-trophy" aria-hidden="true"></i> BEST PAPER NOMINATION</em></strong>
<br/></p>

<p>Tabs have become integral to Web browsing yet have changed little since their
introduction nearly 20 years ago. In contrast, the internet has gone through
dramatic changes and increasingly used to support complex sensemaking tasks.
This paper investigates how tabs today are overloaded with a diverse set of
functionalities and issues users face when managing them. We uncovered
competing pressures pushing for keeping tabs open (ranging from interaction to
emotional costs) versus pushing for closing them (such as limited attention and
resources). We further developed rich design implications for future browser
interfaces.</p>

<!--more-->


<h2>Abstract</h2>

<p>Tabs have become integral to browsing the Web yet have changed little since
their introduction nearly 20 years ago. In contrast, the internet has gone
through dramatic changes, with users increasingly moving from navigating to
websites to exploring information across many sources to support online
sensemaking. This paper investigates how tabs today are overloaded with a
diverse set of functionalities and issues users face when managing them. We
interviewed ten information workers asking about their tab management
strategies and walk through each open tab on their work computers four times
over two weeks. We uncovered competing pressures pushing for keeping tabs open
(ranging from interaction to emotional costs) versus pushing for closing them
(such as limited attention and resources). We then surveyed 103 participants to
estimate the frequencies of these pressures at scale. Finally, we developed
design implications for future browser interfaces that can better support
managing these pressures</p>

<h2>Presentation (SIGCHI 2021)</h2>

<iframe width="560" height="315" src="https://www.youtube.com/embed/pBjIrX9H-Ns" frameborder="0" allowfullscreen></iframe>


<h2>Downloads</h2>

<p><a class="btn btn-default" href="http://josephcc.github.com/images/papers/tabs.pdf" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'TabHoarders', 'PDF']);" role="button">PDF Download</a>
<a class="btn btn-default" href="https://dl.acm.org/doi/10.1145/3411764.3445585" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'TabHoarders', 'ACM']);"  role="button">ACM Digital Library</a></p>

<h2>Media Coverage</h2>

<ul>
<li>Mashable: <a href="https://mashable.com/article/too-many-tabs-open">Stop trying to work in multiple browser tabs. It&#8217;s terrible for your focus.</a></li>
<li>Fast Company: <a href="https://www.fastcompany.com/90635776/the-twisted-psychology-of-browser-tabs-and-why-we-cant-get-rid-of-them">The twisted psychology of browser tabs—and why we can&#8217;t get rid of them</a></li>
<li>MetroNews UK: <a href="https://metro.co.uk/2021/05/10/suffer-from-tab-overload-scientists-study-why-we-have-so-many-open-14540577/amp/">Suffer from &#8220;tab overload&#8221;? Scientists study why we have so many open</a></li>
<li>Inc.: <a href="https://www.inc.com/jessica-stillman/productivity-browser-tabs-carnegie-mellon.html">Stressed Out by Your 87 Open Browser Tabs? New Science Offers a Fix</a></li>
<li>HackerNews: <a href="https://news.ycombinator.com/item?id=27095701">When the Tab Comes Due: Challenges in the Cost Structure of Browser Tab Usage</a></li>
<li>HackerNews: <a href="https://news.ycombinator.com/item?id=27157225">Overcoming tab overload</a></li>
<li>Science Alert: <a href="https://www.sciencealert.com/tab-overload-is-a-common-problem-for-people-browsing-the-internet-survey-finds">We&#8217;re Getting Buried in Browser Tabs And Scientists Want to Fix It</a></li>
<li>EurekAlert: <a href="https://www.eurekalert.org/pub_releases/2021-05/cmu-oto050721.php">Overcoming tab overload</a></li>
<li>NewsGram: <a href="https://www.newsgram.com/skeema-this-tool-will-assist-you-in-managing-your-browser-tabs-more-effectively/">Skeema: This Tool Will Assist You In Managing Your Browser Tabs More Effectively</a></li>
<li>Sify: <a href="https://www.sify.com/news/this-tool-can-help-you-better-manage-browser-tabs-news-education-vfjl5Ebfaiifc.html">This tool can help you better manage browser tabs</a></li>
<li>Revyuh: <a href="https://www.revyuh.com/news/software/apps/browser-tabs-scientists-find-new-way-to-overcome-fear-of-black-hole-effect/">Browser Tabs: Scientists Find New Way to Overcome Fear of Black Hole Effect</a></li>
<li>Carnegie Mellon University: <a href="https://www.cmu.edu/news/stories/archives/2021/may/overcoming-tab-overload.html">Overcoming Tab Overload</a></li>
</ul>


<h2>Citation</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Joseph Chee Chang, Nathan Hahn, Yongsung Kim, Julina Coupland, Bradley
</span><span class='line'>Breneisen, Hannah S Kim, John Hwong, Aniket Kittur. 2021.
</span><span class='line'>When the Tab Comes Due: Challenges in the Cost Structure of Browser Tab Usage.
</span><span class='line'>In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (CHI '21).
</span><span class='line'>ACM, New York, NY, USA, 15 pages. DOI: http://dx.doi.org/10.1145/</span></code></pre></td></tr></table></div></figure>


<h2>Bibtex</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>@inproceedings{Chang:tabs:2021,
</span><span class='line'> author = {Chang, Joseph Chee and Hahn, Nathan and Kim, Yongsung and Coupland,
</span><span class='line'> Julina and Breneisen, Bradley and Kim, Hannah S and Hwong, John and Kittur,
</span><span class='line'> Aniket},
</span><span class='line'> title = {When the Tab Comes Due: Challenges in the Cost Structure of Browser Tab Usage},
</span><span class='line'> booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
</span><span class='line'> series = {CHI '21},
</span><span class='line'> year = {2021},
</span><span class='line'> location = {Online},
</span><span class='line'> numpages = {15},
</span><span class='line'> url = {http://doi.acm.org/10.1145/3411764.3445585},
</span><span class='line'> doi = {10.1145/3411764.3445585},
</span><span class='line'> acmid = {3445585},
</span><span class='line'> publisher = {ACM},
</span><span class='line'> address = {New York, NY, USA},
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mesh]]></title>
    <link href="http://josephcc.github.com/UIST-mesh/"/>
    <updated>2020-10-20T08:00:00-07:00</updated>
    <id>http://josephcc.github.com/UIST-mesh</id>
    <content type="html"><![CDATA[<p>Consumers can choose from many different products and base their decisions on
the tens of thousands of online evidence about each of their options.  However,
to synthesize this information into confident decisions can incur high
interaction and cognitive costs. Online information is scattered across
different sources, and evidence such as reviews can be subjective and
conflicting, requiring users to interpret them under their personal context. We
introduce Mesh, which scaffolds users in iteratively building up a better
understanding of both their choices by evaluating evidence gathered across
sources. Lab and field deployment studies found that Mesh significantly reduces
the costs of gathering and evaluating evidence and scaffolds decision-making
through personalized criteria enabling users to gain deeper insights from data
to make confident purchase decisions.</p>

<!--more-->


<h2>Abstract</h2>

<p>While there is an enormous amount of information online for making decisions
such as choosing a product, restaurant, or school, it can be costly for users
to synthesize that information into confident decisions. Information for users&#8217;
many different criteria needs to be gathered from many different sources into a
structure where they can be compared and contrasted. The usefulness of each
criterion for differentiating potential options can be opaque to users, and
evidence such as reviews may be subjective and conflicting, requiring users to
interpret each under their personal context. We introduce Mesh, which
scaffolds users in iteratively building up a better understanding of both their
criteria and options by evaluating evidence gathered across sources in the
context of consumer decision making. Mesh bridges the gap between decision
support systems that typically have rigid structures and the fluid and dynamic
process of exploratory search, changing the cost structure to provide
increasing payoffs with greater user investment. Our lab and field deployment
studies found evidence that Mesh significantly reduces the costs of
gathering and evaluating evidence and scaffolds decision-making through
personalized criteria enabling users to gain deeper insights from data.</p>

<h2>Video Figure</h2>

<h4>5-minute virtual conference presentation</h4>

<iframe width="560" height="315" src="https://www.youtube.com/embed/LNASh9rq9-I?rel=0" frameborder="0" allowfullscreen></iframe>


<h4>3-minute video abstract</h4>

<iframe width="560" height="315" src="https://www.youtube.com/embed/NqriHlTfVhU?rel=0" frameborder="0" allowfullscreen></iframe>


<h2>Download</h2>

<p><a class="btn btn-default" href="http://josephcc.github.com/images/papers/mesh.pdf" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Mesh', 'PDF']);" role="button">PDF Download</a>
<a class="btn btn-default" href="https://dl.acm.org/doi/10.1145/3379337.3415865" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Mesh', 'ACM']);"  role="button">ACM Digital Library</a></p>

<h2>Citation</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Joseph Chee Chang, Nathan Hahn, Aniket Kittur.
</span><span class='line'>Mesh: Scaffolding Comparison Tables for Online Decision Making.
</span><span class='line'>In Proceedings of the 33rd ACM User Interface Software and Technology Symposium: UIST 2020.</span></code></pre></td></tr></table></div></figure>


<h2>Bibtex</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>@article{chang2020mesh,
</span><span class='line'>  title={Mesh: Scaffolding Comparison Tables for Online Decision Making},
</span><span class='line'>  author={Chang, Joseph Chee and Hahn, Nathan, and Kittur, Aniket},
</span><span class='line'>  booktitle = {Proceedings of the 33rd ACM User Interface Software and Technology Symposium},
</span><span class='line'>  series = {UIST'20},
</span><span class='line'>  year={2020}
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ph.D Thesis]]></title>
    <link href="http://josephcc.github.com/CMU-thesis/"/>
    <updated>2020-06-02T08:00:00-07:00</updated>
    <id>http://josephcc.github.com/CMU-thesis</id>
    <content type="html"><![CDATA[<!--more-->


<p><a class="btn btn-default" href="http://josephcc.github.com/Thesis.pdf" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Thesis', 'PDF']);" role="button">PDF Download</a></p>

<iframe src="http://josephcc.github.com/Thesis.pdf" style='width: 100%; height: 800px; border: 1px darkgray solid;'>
  Thesis
</iframe>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SearchLens]]></title>
    <link href="http://josephcc.github.com/IUI-searchlens/"/>
    <updated>2019-03-17T08:00:00-07:00</updated>
    <id>http://josephcc.github.com/IUI-searchlens</id>
    <content type="html"><![CDATA[<p>Whether figuring out where to eat in an unfamiliar city or deciding which
apartment to live in, reviews and forum posts are often a significant factor in
online decision making. However, making sense of these rich repositories of
diverse opinions can be prohibitively effortful, searchers need to sift through
a large number of reviews to characterize each item based on aspects that they
care about. We introduce a novel system, SearchLens, where searchers build up a
collection of composable and reusable &#8220;Lenses&#8221; that reflect their different
latent interests. Also, the Lenses allowed the system to generate personalized
interfaces with visual explanations that promote transparency and enable
in-depth exploration.</p>

<!--more-->


<h2>Abstract</h2>

<p>Whether figuring out where to eat in an unfamiliar city or deciding which
apartment to live in, consumer generated data (i.e. reviews and forum posts)
are often an important influence in online decision making. To make sense of
these rich repositories of diverse opinions, searchers need to sift through a
large number of reviews to characterize each item based on aspects that they
care about. We introduce a novel system, SearchLens, where searchers build up a
collection of &#8220;Lenses&#8221; that reflect their different latent interests, and
compose the Lenses to find relevant items across different contexts. Based on
the Lenses, SearchLens generates personalized interfaces with visual
explanations that promotes transparency and enables deeper exploration. While
prior work found searchers may not wish to put in effort specifying their goals
without immediate and sufficient benefits, results from a controlled lab study
suggest that our approach incentivized participants to express their interests
more richly than in a baseline condition, and a field study showed that
participants found benefits in SearchLens while conducting their own tasks.</p>

<h2>Video Figure</h2>

<iframe width="560" height="315" src="https://www.youtube.com/embed/dXcTtHMa2DQ?rel=0" frameborder="0" allowfullscreen></iframe>


<h2>Download</h2>

<p><a class="btn btn-default" href="http://josephcc.github.com/images/papers/searchlens.pdf" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'SearchLens', 'PDF']);" role="button">PDF Download</a>
<a class="btn btn-default" href="https://dl.acm.org/citation.cfm?id=3302321" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'SearchLens', 'ACM']);"  role="button">ACM Digital Library</a></p>

<h2>Citation</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>SearchLens: Composing and Capturing Complex User Interests for Exploratory Search.
</span><span class='line'>Joseph Chee Chang, Nathan Hahn, Adam Perer, Aniket Kittur.
</span><span class='line'>In Proceedings of the 2019 ACM 24th Annual Meeting of the Intelligent User Interfaces: IUI 2019.</span></code></pre></td></tr></table></div></figure>


<h2>Bibtex</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>@article{chang2019searchlens,
</span><span class='line'>  title={SearchLens: Composing and Capturing Complex User Interests for Exploratory Search},
</span><span class='line'>  author={Chang, Joseph Chee and Hahn, Nathan, and Perer, Adam and Kittur, Aniket},
</span><span class='line'>  booktitle = {Proceedings of the 2019 ACM 24th Annual Meeting of the Intelligent User Interfaces},
</span><span class='line'>  series = {IUI'19},
</span><span class='line'>  year={2019}
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Solvent]]></title>
    <link href="http://josephcc.github.com/CSCW-solvent/"/>
    <updated>2018-11-03T08:00:00-07:00</updated>
    <id>http://josephcc.github.com/CSCW-solvent</id>
    <content type="html"><![CDATA[<p>Analogies in distant domains often lead to scientific discoveries. However, it
can be prohibitively difficult for researchers to find useful analogies from
unfamiliar domains as search engines poorly support it. We introduce Solvent, a
mixed-initiative system where annotators structure abstracts of academic papers
into different aspects and use a semantic model to find analogies among
research papers and across different domains. These results demonstrate a new
path towards computationally supported knowledge sharing in research
communities.</p>

<!--more-->


<h2>Abstract</h2>

<p>Scientific discoveries are often driven by finding analogies in distant
domains, but the growing number of papers makes it difficult to find relevant
ideas in a single discipline, let alone distant analogies in other domains. To
provide computational support for finding analogies across domains, we
introduce Solvent, a mixed-initiative system where humans annotate aspects of
research papers that denote their background (the high-level problems being
addressed), purpose (the specific problems being addressed), mechanism (how
they achieved their purpose), and findings (what they learned/achieved), and a
computational model constructs a semantic representation from these annotations
that can be used to find analogies among the research papers. We demonstrate
that this system finds more analogies than baseline information-retrieval
approaches; that annotators and annotations can generalize beyond domain; and
that the resulting analogies found are useful to experts. These results
demonstrate a novel path towards computationally supported knowledge sharing in
research communities.</p>

<h2>Downloads</h2>

<p><a class="btn btn-default" href="http://josephcc.github.com/images/papers/solvent.pdf" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Solvent', 'PDF']);" role="button">PDF Download</a></p>

<h2>Citation</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Joel Chan, Joseph Chee Chang, Tom Hope, Dafna Shahaf, Aniket Kittur. 2018.
</span><span class='line'>Solvent: A Mixed Initiative System for Finding Analogies between Research Papers
</span><span class='line'>In Proceedings of the 2018 ACM Human-Computer Interaction: CSCW 2018.</span></code></pre></td></tr></table></div></figure>


<h2>Bibtex</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>@article{chan2018solvent,
</span><span class='line'>  title={SOLVENT: A Mixed Initiative System for Finding Analogies between Research Papers},
</span><span class='line'>  author={CHAN, JOEL and CHANG, JOSEPH CHEE and HOPE, TOM and SHAHAF, DAFNA and KITTUR, ANIKET},
</span><span class='line'>  booktitle = {Proceedings of the 2018 ACM Human-Computer Interaction: CSCW},
</span><span class='line'>  series = {CSCW'18},
</span><span class='line'>  year={2018}
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Bento Browser]]></title>
    <link href="http://josephcc.github.com/CHI-bento/"/>
    <updated>2018-04-21T08:01:00-07:00</updated>
    <id>http://josephcc.github.com/CHI-bento</id>
    <content type="html"><![CDATA[<p>Complex searches can be overwhelming, leading to lots of opened tabs. This tab overload can
make conducting searches on mobile devices especially difficult where screen
real-estate is limited, and progress can often be interrupted. Rather than
using tabs to manage information, we introduce browsing through scaffolding.
Search result lists serve as mutable workspaces where progress can be suspended
and resumed. BentoBrowser is available for <a href="https://itunes.apple.com/us/app/bento-browser/id1101530325?mt=8">download from the iPhone AppStore</a>.</p>

<!--more-->


<h2>Abstract</h2>

<p>People engaged in complex searches such as planning a vacation or understanding
their medical symptoms are often overwhelmed by opening and managing many tabs.
These challenges are exacerbated as search moves to smartphones and mobile
devices where screen real-estate is limited and tasks are frequently suspended,
resumed, and interleaved. Rather than continue to utilize tab-based browsing
for complex search, we introduce a new way of browsing through a scaffolded
interface. The list of search results serves as a mutable workspace, where a
user can track progress on a specific information query. The search query
serves as a gateway into this workspace, accessed through a task-subtask
hierarchy. We instantiate this in the Bento mobile search system and
investigate its effectiveness in three studies. We find converging evidence
that users were able to make progress on their complex searching tasks with
this structure, and find it more organized and easier to revisit.</p>

<h2>Install Bento Browser</h2>

<p>The version on the AppStore is the third iteration of the version from the CHI 2018 paper.</p>

<p><img src="http://josephcc.github.com/images/projects/bento_screenshots.png" style='width: 100%;'/></p>

<p><a class="btn btn-default" href="https://itunes.apple.com/us/app/bento-browser/id1101530325?mt=8" target='_blank' onclick="_gaq.push(['_trackEvent', 'Demo', 'Bento', 'AppStore']);" role="button">iPhone App (AppStore)</a></p>

<h2>Video Demo</h2>

<p>The version in the video is the second iteration of the version from the CHI 2018 paper.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/fm5zK1vm1Q8?rel=0" frameborder="0" allowfullscreen></iframe>


<h2>Media Coverage</h2>

<ul>
<li>Tech Explorist: <a href="https://www.techexplorist.com/new-web-browser-easier-search-mobile-devices/13794/">A new web browser makes it easier to search on mobile devices</a></li>
<li>THE Journal: <a href="https://thejournal.com/articles/2018/05/02/carnegie-mellons-bento-browser-organizes-complex-mobile-searches.aspx">New Bento Browser Organizes Complex Mobile Searches</a></li>
<li>Science Daily: <a href="https://www.sciencedaily.com/releases/2018/04/180425093747.htm">Bento browser makes it easier to search on mobile devices</a></li>
<li>Prikk: <a href="https://www.prikk.world/en/news/wirtschaft/innovation/bento-erleichtert-websuche-auf-mobilgeraten">&#8220;BENTO&#8221; ERLEICHTERT WEBSUCHE AUF MOBILGERÄTEN</a></li>
<li>CMU News: <a href="https://www.cmu.edu/news/stories/archives/2018/april/bento-browser.html">Bento Browser Makes it Easier To Search on Mobile Devices</a></li>
<li>CMU HCII: <a href="https://hcii.cmu.edu/news/2018/bento-browser-makes-it-easier-search-mobile-devices">Bento Browser Makes it Easier To Search on Mobile Devices</a></li>
</ul>


<h2>Downloads</h2>

<p><a class="btn btn-default" href="http://josephcc.github.com/images/papers/bento.pdf" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Evorus', 'PDF']);" role="button">PDF Download</a></p>

<h2>Citation</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Nathan Hahn, Joseph Chee Chang, Aniket Kittur. 2018.
</span><span class='line'>Bento Browser: Complex Mobile Search Without Tabs.
</span><span class='line'>In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI '18).
</span><span class='line'>ACM, Montreal, QC, Canada.</span></code></pre></td></tr></table></div></figure>


<h2>Bibtex</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>@inproceedings{Hahn:2018:Bento,
</span><span class='line'> author = {Hahn, Nathan and Chang, Joseph Chee and Kittur, Aniket},
</span><span class='line'> title = {Bento Browser: Complex Mobile Search Without Tabs.},
</span><span class='line'> booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
</span><span class='line'> series = {CHI '18},
</span><span class='line'> year = {2018},
</span><span class='line'> publisher = {ACM},
</span><span class='line'> address = {Montreal, QC, Canada},
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Evorus]]></title>
    <link href="http://josephcc.github.com/CHI-evorus/"/>
    <updated>2018-04-21T08:00:00-07:00</updated>
    <id>http://josephcc.github.com/CHI-evorus</id>
    <content type="html"><![CDATA[<p><strong><em><i class="fa fa-trophy" aria-hidden="true"></i> BEST PAPER NOMINATION</em></strong>
<br/>
Crowd-powered chatbots are robust than current pure AI approach, but can be
slower and more expensive at runtime. We attempted to combine the two
approaches for high quality, low latency, and low cost.  We introduce Evorus, a
crowd-powered chatbot that automate itself over time by learning to integrate
AI chatbots, reusing responses, and assess response quality. A 5-month-long
public deployment study shows promising results. You can <a href="http://talkingtothecrowd.org/">try talking to Evorus today</a>.</p>

<!--more-->


<h2>Abstract</h2>

<p>Crowd-powered conversational assistants have been shown to be more robust than
automated systems, but do so at the cost of higher response latency and
monetary costs.  A promising direction is to combine the two approaches for
high quality, low latency, and low cost solutions.  In this paper, we introduce
Evorus, a crowd-powered conversational assistant built to automate itself over
time by i) allowing new chatbots to be easily integrated to automate more
scenarios, ii) reusing prior crowd answers, and iii) learning to automatically
approve response candidates.  Our 5-month-long deployment with 80 participants
and 281 conversations shows that Evorus can automate itself without
compromising conversation quality.  Crowd-AI architectures have long been
proposed as a way to reduce cost and latency for crowd-powered systems; Evorus
demonstrates how automation can be introduced successfully in a deployed
system. Its architecture allows future researchers to make further innovation
on the underlying automated components in the context of a deployed open domain
dialog system.</p>

<h2>Try Evorus on Google Talk</h2>

<p><a class="btn btn-default" href="http://talkingtothecrowd.org/" target='_blank' onclick="_gaq.push(['_trackEvent', 'Demo', 'Evorus', 'Website']);" role="button">TalkingToTheCrowd.org</a></p>

<h2>Overview Video</h2>

<iframe width="560" height="315" src="https://www.youtube.com/embed/3SAG8jP-Q-M?rel=0" frameborder="0" allowfullscreen></iframe>


<h2>Media Coverage</h2>

<ul>
<li>WTAE TV News: <a href="http://www.wtae.com/article/chorus-chatbot-carnegie-mellon-university-pittsburgh/16870459">Meet the &#8216;Chorus&#8217; chatbot: Unlike Alexa or Siri, it&#8217;s powered by actual people on the other end</a></li>
<li>Trib Live: <a href="http://triblive.com/business/technology/13275597-74/chatbot-developed-at-carnegie-mellon-uses-humans-to-answer-questions-ais-cant">Chatbot developed at Carnegie Mellon uses humans to answer questions AIs can&#8217;t</a></li>
<li>EurekaAlert!: <a href="https://www.eurekalert.org/pub_releases/2018-02/cmu-cwa020618.php">Crowd workers, AI make conversational agents smarter</a></li>
<li>CMU News: <a href="https://www.cs.cmu.edu/news/crowd-workers-ai-make-conversational-agents-smarter">Crowd workers, AI make conversational agents smarter</a></li>
<li>Presstext: <a href="https://www.pressetext.com/#news/20180208014">KI-System &#8220;Evorus&#8221; wird zunehmend eigenständig</a></li>
</ul>


<h2>Downloads</h2>

<p><a class="btn btn-default" href="http://josephcc.github.com/images/papers/evorus.pdf" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Evorus', 'PDF']);" role="button">PDF Download</a>
<a class="btn btn-default" href="https://arxiv.org/abs/1801.02668" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Evorus', 'arXiv']);" role="button">arXiv</a></p>

<h2>Citation</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Kenneth Huang, Joseph Chee Chang, Jeffrey P. Bigham. 2018.
</span><span class='line'>Evorus: A Crowd-powered Conversational AssistantBuilt to Automate Itself Over Time.
</span><span class='line'>In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI '18).
</span><span class='line'>ACM, Montreal, QC, Canada.</span></code></pre></td></tr></table></div></figure>


<h2>Bibtex</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>@inproceedings{Huang:2018:Evorus,
</span><span class='line'> author = {Huang, Kenneth and Chang, Joseph Chee and Bigham, Jeffrey P.},
</span><span class='line'> title = {Evorus: A Crowd-powered Conversational AssistantBuilt to Automate Itself Over Time},
</span><span class='line'> booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
</span><span class='line'> series = {CHI '18},
</span><span class='line'> year = {2018},
</span><span class='line'> publisher = {ACM},
</span><span class='line'> address = {Montreal, QC, Canada},
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Revolt]]></title>
    <link href="http://josephcc.github.com/CHI-revolt/"/>
    <updated>2017-05-06T08:00:00-07:00</updated>
    <id>http://josephcc.github.com/CHI-revolt</id>
    <content type="html"><![CDATA[<p>Generating comprehensive labeling guidelines for crowdworkers can be
challenging for complex datasets.  Revolt harnesses <em>crowd disagreements</em>
to identify ambiguous concepts in the data and coordinates the crowd to
<em>collaboratively</em> create rich structures for requesters to make posthoc
decisions, removing the need for comprehensive guidelines and
enabling dynamic label boundaries.</p>

<p><span style='color: gray'>Work done during internship at Microsoft Research, Redmond.</span></p>

<!--more-->


<h2>Abstract</h2>

<p>Crowdsourcing provides a scalable and efficient way to construct labeled
datasets for training machine learning systems. However, creating comprehensive
label guidelines for crowdworkers is often prohibitive even for seemingly
simple concepts. Incomplete or ambiguous label guidelines can then result in
differing interpretations of concepts and inconsistent labels. Existing
approaches for improving label quality, such as worker screening or detection
of poor work, are ineffective for this problem and can lead to rejection of
honest work and a missed opportunity to capture rich interpretations about
data. We introduce Revolt, a collaborative approach that brings ideas from
expert annotation workflows to crowd-based labeling. Revolt eliminates the
burden of creating detailed label guidelines by harnessing crowd disagreements
to identify ambiguous concepts and create rich structures (groups of
semantically related items) for post-hoc label decisions. Experiments comparing
Revolt to traditional crowdsourced labeling show that Revolt produces high
quality labels without requiring label guidelines in turn for an increase in
monetary cost. This up front cost, however, is mitigated by Revolt&#8217;s ability to
produce reusable structures that can accommodate a variety of label boundaries
without requiring new data to be collected. Further comparisons of Revolt&#8217;s
collaborative and non-collaborative variants show that collaboration reaches
higher label accuracy with lower monetary cost.</p>

<h2>Downloads</h2>

<p><a class="btn btn-default" href="http://josephcc.github.com/images/papers/revolt-crowd-labeling.pdf" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Revolt', 'PDF']);" role="button">PDF Download</a>
<a class="btn btn-default" href="http://dl.acm.org/citation.cfm?id=3026044" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Revolt', 'ACM']);" role="button">ACM Digital Library</a>
<a class="btn btn-default" href="http://josephcc.github.com/images/papers/revolt-notes.pdf" target='_blank' onclick="_gaq.push(['_trackEvent', 'Notes', 'Revolt', 'PDF']);" role="button">Technical and Design Notes (draft)</a></p>

<h2>Citation</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Joseph Chee Chang, Saleema Amershi, and Ece Kamar. 2017.
</span><span class='line'>Revolt: Collaborative Crowdsourcing for Labeling Machine Learning Datasets.
</span><span class='line'>In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI '17).
</span><span class='line'>ACM, New York, NY, USA, 3180-3191. DOI: http://dx.doi.org/10.1145/3025453.3026044</span></code></pre></td></tr></table></div></figure>


<h2>Bibtex</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>@inproceedings{Chang:2017:Revolt,
</span><span class='line'> author = {Chang, Joseph Chee and Amershi, Saleema and Kamar, Ece},
</span><span class='line'> title = {Revolt: Collaborative Crowdsourcing for Labeling Machine Learning Datasets},
</span><span class='line'> booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
</span><span class='line'> series = {CHI '17},
</span><span class='line'> year = {2017},
</span><span class='line'> url = {http://doi.acm.org/10.1145/3025453.3026044},
</span><span class='line'> doi = {10.1145/3025453.3026044},
</span><span class='line'> publisher = {ACM},
</span><span class='line'> address = {New York, NY, USA},
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Intentionally Uncertain Input]]></title>
    <link href="http://josephcc.github.com/UIST-highlight/"/>
    <updated>2016-10-16T08:00:00-07:00</updated>
    <id>http://josephcc.github.com/UIST-highlight</id>
    <content type="html"><![CDATA[<p>Highlighting can be mentally taxing for learners who are often unsure about how
much information they needed to include.  We introduce the idea of
intentionally uncertain input in the context of highlighting on mobile devices.
We present a system that uses force touch and fuzzy bounding boxes to support
saving information while users are uncertain about where to highlight.</p>

<!--more-->


<h2>Abstract</h2>

<p>Patients researching medical diagnoses, scientist exploring new fields of
literature, and students learning about new domains are all faced with the
challenge of capturing information they find for later use. However, saving
information is challenging on mobile devices, where the small screen and font
sizes combined with the inaccuracy of finger based touch screens makes it time
consuming and stressful for people to select and save text for future use.
Furthermore, beyond the challenge of simply selecting a region of bounded text
on a mobile device, in many learning and data exploration tasks the boundaries
of what text may be relevant and useful later are themselves uncertain for the
user. In contrast to previous approaches which focused on speeding up the
selection process by making the identification of hard boundaries faster, we
introduce the idea of intentionally supporting uncertain input in the context
of saving information during complex reading and information exploration. We
embody this idea in a system that uses force touch and fuzzy bounding boxes
along with posthoc expandable context to support identifying and saving
information in an intentionally uncertain way on mobile devices. In a two part
user study we find that this approach reduced selection time and was preferred
by participants over the default system text selection method.</p>

<h2>Presentation (UIST 2016)</h2>

<iframe width="560" height="315" src="https://www.youtube.com/embed/HPG6NWRTGvY?list=PLqhXYFYmZ-VcUPus2QYpAZdFmw5w6seVR" frameborder="0" allowfullscreen></iframe>


<h2>Demo Video</h2>

<iframe width="560" height="315" src="https://www.youtube.com/embed/Rj_0GFeUQjg?rel=0" frameborder="0" allowfullscreen></iframe>


<h2>Downloads</h2>

<p><a class="btn btn-default" href="http://josephcc.github.com/images/papers/mobile-highlighting.pdf" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Highlight', 'PDF']);" role="button">PDF Download</a>
<a class="btn btn-default" href="http://dl.acm.org/citation.cfm?id=2984538" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Highlight', 'ACM']);"  role="button">ACM Digital Library</a></p>

<h2>Citation</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Joseph Chee Chang, Nathan Hahn, and Aniket Kittur. 2016.
</span><span class='line'>Supporting Mobile Sensemaking Through Intentionally Uncertain Highlighting.
</span><span class='line'>In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST '16).
</span><span class='line'>ACM, New York, NY, USA, 61-68. DOI: http://dx.doi.org/10.1145/2984511.2984538</span></code></pre></td></tr></table></div></figure>


<h2>Bibtex</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>@inproceedings{Chang:2016:SMS:2984511.2984538,
</span><span class='line'> author = {Chang, Joseph Chee and Hahn, Nathan and Kittur, Aniket},
</span><span class='line'> title = {Supporting Mobile Sensemaking Through Intentionally Uncertain Highlighting},
</span><span class='line'> booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
</span><span class='line'> series = {UIST '16},
</span><span class='line'> year = {2016},
</span><span class='line'> isbn = {978-1-4503-4189-9},
</span><span class='line'> location = {Tokyo, Japan},
</span><span class='line'> pages = {61--68},
</span><span class='line'> numpages = {8},
</span><span class='line'> url = {http://doi.acm.org/10.1145/2984511.2984538},
</span><span class='line'> doi = {10.1145/2984511.2984538},
</span><span class='line'> acmid = {2984538},
</span><span class='line'> publisher = {ACM},
</span><span class='line'> address = {New York, NY, USA},
</span><span class='line'> keywords = {annotation, capture, copy-paste, highlighting., information, saving, sensemaking},
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Alloy]]></title>
    <link href="http://josephcc.github.com/CHI-alloy/"/>
    <updated>2016-05-07T08:00:00-07:00</updated>
    <id>http://josephcc.github.com/CHI-alloy</id>
    <content type="html"><![CDATA[<p><strong><em><i class="fa fa-trophy" aria-hidden="true"></i> BEST PAPER NOMINATION</em></strong>
<br/>
<strong><em>HCOMP 2016 INVITED ENCORE TALK</em></strong>
<br/>
Many crowd clustering approaches have difficulties providing global context to
workers in order to generate meaningful categories. Alloy uses a
<em>sample-and-search</em> technique to provide a better understanding of the global
context. It also combines the in-depth semantic knowledge from human
computation and the scalability of machine learning models to create rich
structures from unorganized documents with high quality and efficiency.</p>

<!--more-->


<h2>Abstract</h2>

<p>Crowdsourced clustering approaches present a promising way to harness deep
semantic knowledge for clustering complex information. However, existing
approaches have difficulties supporting the global context needed for workers
to generate meaningful categories, and are costly because all items require
human judgments. We introduce Alloy, a hybrid approach that combines the
richness of human judgments with the power of machine algorithms. Alloy
supports greater global context through a new <em>sample and search</em>
crowd pattern which changes the crowd&#8217;s task from classifying a fixed subset of
items to actively sampling and querying the entire dataset.  It also improves
efficiency through a two phase process in which crowds provide examples to help
a machine cluster the head of the distribution, then classify low-confidence
examples in the tail. To accomplish this, Alloy introduces a modular
<em>cast and gather</em> approach which leverages a machine learning backbone
to stitch together different types of judgment tasks.</p>

<h2>Downloads</h2>

<p><a class="btn btn-default" href="http://josephcc.github.com/images/papers/alloy-crowd-clustering.pdf" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Alloy', 'PDF']);" role="button">PDF Download</a>
<a class="btn btn-default" href="http://dl.acm.org/citation.cfm?id=2858411" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Alloy', 'ACM']);" role="button">ACM Digital Library</a></p>

<h2>Media Coverage</h2>

<ul>
<li>Pittsburgh Post-Gazette: <a href="http://www.post-gazette.com/business/tech-news/2016/05/11/Crowdsourcing-work-Get-rid-of-the-human-supervisor-CMU/stories/201605100127">Crowdsourcing work? Get rid of the human supervisor</a></li>
<li>Campus Technology: <a href="https://campustechnology.com/articles/2016/05/10/research-project-mixes-humans-and-machines-for-better-crowdsourcing.aspx">Research Project Mixes Humans and Machines for Better Crowdsourcing</a></li>
<li>Neuraoscience News: <a href="http://neurosciencenews.com/human-machine-intelligence-framework-4221/">Crowd Augmented Cognition: Combining Human and Machine Intelligence to Accelerate Learning</a></li>
<li>DZone: <a href="https://dzone.com/articles/researchers-work-on-automated-means-of-managing-th">Research Suggests AI Managers Effective for Crowdsourcing</a></li>
<li>PhysOrg: <a href="https://phys.org/news/2016-05-crowd-augmented-cognition-team-tools-combine.html">Crowd-augmented cognition: Team develops tools that combine human and machine intelligence to accelerate learning</a></li>
<li>TechExplore: <a href="https://techxplore.com/news/2016-05-big-small-pieces-humans-crowdsourced.html">Big thinking in small pieces: Computer guides humans in crowdsourced research</a></li>
<li>Spend Matters: <a href="http://spendmatters.com/2016/06/09/crowdsourcing-and-cognitive-computing-are-you-ready-for-the-future-of-work/">Crowdsourcing and Cognitive Computing: Are You Ready for the Future of Work?</a></li>
<li>Science Daily: <a href="https://www.sciencedaily.com/releases/2016/05/160511210628.htm">Crowd-augmented cognition - combine human, machine intelligence to accelerate learning</a></li>
<li>EurekAlert: <a href="https://www.eurekalert.org/pub_releases/2016-05/cmu-bti051016.php">Big thinking in small pieces: Computer guides humans in crowdsourced research</a></li>
<li>EurekAlert: <a href="https://www.eurekalert.org/pub_releases/2016-05/nsf-cc051116.php">Crowd-augmented cognition</a></li>
<li>NSF News: <a href="https://www.nsf.gov/news/mmg/mmg_disp.jsp?med_id=80586&amp;from=">Big thinking in small pieces: Computer guides humans in crowdsourced research</a></li>
<li>NSF News: <a href="https://www.nsf.gov/news/news_summ.jsp?cntn_id=138580&amp;org=NSF">Crowd-augmented cognition</a></li>
<li>CMU SCS News: <a href="https://www.cmu.edu/news/stories/archives/2016/may/knowledge-accelerator.html">Big thinking in small pieces: Computer guides humans in crowdsourced research</a></li>
<li>CMU HCII News: <a href="https://hcii.cmu.edu/news/2016/hcii-chi-computer-guides-humans-crowdsourced-research">Computer Guides Humans in Crowdsourced Research</a></li>
</ul>


<h2>Citation</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Joseph Chee Chang, Aniket Kittur, and Nathan Hahn. 2016.
</span><span class='line'>Alloy: Clustering with Crowds and Computation.
</span><span class='line'>In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI '16).
</span><span class='line'>ACM, New York, NY, USA, 3180-3191. DOI: http://dx.doi.org/10.1145/2858036.2858411</span></code></pre></td></tr></table></div></figure>


<h2>Bibtex</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>@inproceedings{Chang:2016:ACC:2858036.2858411,
</span><span class='line'> author = {Chang, Joseph Chee and Kittur, Aniket and Hahn, Nathan},
</span><span class='line'> title = {Alloy: Clustering with Crowds and Computation},
</span><span class='line'> booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
</span><span class='line'> series = {CHI '16},
</span><span class='line'> year = {2016},
</span><span class='line'> isbn = {978-1-4503-3362-7},
</span><span class='line'> location = {Santa Clara, California, USA},
</span><span class='line'> pages = {3180--3191},
</span><span class='line'> numpages = {12},
</span><span class='line'> url = {http://doi.acm.org/10.1145/2858036.2858411},
</span><span class='line'> doi = {10.1145/2858036.2858411},
</span><span class='line'> acmid = {2858411},
</span><span class='line'> publisher = {ACM},
</span><span class='line'> address = {New York, NY, USA},
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Knowledge Accelorator]]></title>
    <link href="http://josephcc.github.com/CHI-ka/"/>
    <updated>2016-05-07T07:00:00-07:00</updated>
    <id>http://josephcc.github.com/CHI-ka</id>
    <content type="html"><![CDATA[<p><strong><em><i class="fa fa-trophy" aria-hidden="true"></i> BEST PAPER NOMINATION</em></strong>
<br/>
Answering complex questions such as &#8220;How do I grow better tomatoes?&#8221; often
requires individuals to conduct extensive online research and synthesis. Can we
crowdsource this complex, high context process with 100 crowdworkers conducting
microtasks distributedly? The Knowledge Accelerator uses crowdworkers to
extract and synthesize text clips across web pages into coherent articles
without a centralized coordinator.</p>

<!--more-->


<h2>Abstract</h2>

<p>Crowdsourcing  offers  a  powerful  new  paradigm  for  onlinework.   However,
real  world  tasks  are  often  interdependent,requiring a big picture view of
the difference pieces involved. Existing  crowdsourcing  approaches  that
support  such  tasks &#8211; ranging from Wikipedia to flash teams &#8211; are
bottleneckedby relying on a small number of individuals to maintain thebig
picture.   In this paper,  we explore the idea that a computational system can
scaffold an emerging interdependent,big picture view entirely through the small
contributions ofindividuals, each of whom sees only a part of the whole.
Toinvestigate the viability, strengths, and weaknesses of this approach we
instantiate the idea in a prototype system for accomplishing  distributed
information  synthesis  and  evaluateits output across a variety of topics.  We
also contribute a setof design patterns that may be informative for other
systemsaimed at supporting big picture thinking in small pieces.</p>

<h2>Downloads</h2>

<p><a class="btn btn-default" href="http://josephcc.github.com/images/papers/knowledge-accelorator.pdf" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'KA', 'PDF']);" role="button">PDF Download</a>
<a class="btn btn-default" href="http://dl.acm.org/citation.cfm?id=2858364" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'KA', 'ACM']);"  role="button">ACM Digital Library</a></p>

<h2>Media Coverage</h2>

<ul>
<li>Pittsburgh Post-Gazette: <a href="http://www.post-gazette.com/business/tech-news/2016/05/11/Crowdsourcing-work-Get-rid-of-the-human-supervisor-CMU/stories/201605100127">Crowdsourcing work? Get rid of the human supervisor</a></li>
<li>Campus Technology: <a href="https://campustechnology.com/articles/2016/05/10/research-project-mixes-humans-and-machines-for-better-crowdsourcing.aspx">Research Project Mixes Humans and Machines for Better Crowdsourcing</a></li>
<li>Neuraoscience News: <a href="http://neurosciencenews.com/human-machine-intelligence-framework-4221/">Crowd Augmented Cognition: Combining Human and Machine Intelligence to Accelerate Learning</a></li>
<li>DZone: <a href="https://dzone.com/articles/researchers-work-on-automated-means-of-managing-th">Research Suggests AI Managers Effective for Crowdsourcing</a></li>
<li>PhysOrg: <a href="https://phys.org/news/2016-05-crowd-augmented-cognition-team-tools-combine.html">Crowd-augmented cognition: Team develops tools that combine human and machine intelligence to accelerate learning</a></li>
<li>TechExplore: <a href="https://techxplore.com/news/2016-05-big-small-pieces-humans-crowdsourced.html">Big thinking in small pieces: Computer guides humans in crowdsourced research</a></li>
<li>Spend Matters: <a href="http://spendmatters.com/2016/06/09/crowdsourcing-and-cognitive-computing-are-you-ready-for-the-future-of-work/">Crowdsourcing and Cognitive Computing: Are You Ready for the Future of Work?</a></li>
<li>Science Daily: <a href="https://www.sciencedaily.com/releases/2016/05/160511210628.htm">Crowd-augmented cognition - combine human, machine intelligence to accelerate learning</a></li>
<li>EurekAlert: <a href="https://www.eurekalert.org/pub_releases/2016-05/cmu-bti051016.php">Big thinking in small pieces: Computer guides humans in crowdsourced research</a></li>
<li>EurekAlert: <a href="https://www.eurekalert.org/pub_releases/2016-05/nsf-cc051116.php">Crowd-augmented cognition</a></li>
<li>NSF News: <a href="https://www.nsf.gov/news/mmg/mmg_disp.jsp?med_id=80586&amp;from=">Big thinking in small pieces: Computer guides humans in crowdsourced research</a></li>
<li>NSF News: <a href="https://www.nsf.gov/news/news_summ.jsp?cntn_id=138580&amp;org=NSF">Crowd-augmented cognition</a></li>
<li>CMU SCS News: <a href="https://www.cmu.edu/news/stories/archives/2016/may/knowledge-accelerator.html">Big thinking in small pieces: Computer guides humans in crowdsourced research</a></li>
<li>CMU HCII News: <a href="https://hcii.cmu.edu/news/2016/hcii-chi-computer-guides-humans-crowdsourced-research">Computer Guides Humans in Crowdsourced Research</a></li>
</ul>


<h2>Citation</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Nathan Hahn, Joseph Chang, Ji Eun Kim, and Aniket Kittur. 2016.
</span><span class='line'>The Knowledge Accelerator: Big Picture Thinking in Small Pieces.
</span><span class='line'>In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI '16).
</span><span class='line'>ACM, New York, NY, USA, 2258-2270. DOI: http://dx.doi.org/10.1145/2858036.2858364</span></code></pre></td></tr></table></div></figure>


<h2>Bibtex</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>@inproceedings{Hahn:2016:KAB:2858036.2858364,
</span><span class='line'> author = {Hahn, Nathan and Chang, Joseph and Kim, Ji Eun and Kittur, Aniket},
</span><span class='line'> title = {The Knowledge Accelerator: Big Picture Thinking in Small Pieces},
</span><span class='line'> booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
</span><span class='line'> series = {CHI '16},
</span><span class='line'> year = {2016},
</span><span class='line'> isbn = {978-1-4503-3362-7},
</span><span class='line'> location = {Santa Clara, California, USA},
</span><span class='line'> pages = {2258--2270},
</span><span class='line'> numpages = {13},
</span><span class='line'> url = {http://doi.acm.org/10.1145/2858036.2858364},
</span><span class='line'> doi = {10.1145/2858036.2858364},
</span><span class='line'> acmid = {2858364},
</span><span class='line'> publisher = {ACM},
</span><span class='line'> address = {New York, NY, USA},
</span><span class='line'> keywords = {complex workflow, crowd work, crowdsourcing, design patterns, information synthesis},
</span><span class='line'>}
</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Twitter Code-Switching]]></title>
    <link href="http://josephcc.github.com/arXiv-twitter-code-switching/"/>
    <updated>2014-12-22T08:00:00-08:00</updated>
    <id>http://josephcc.github.com/arXiv-twitter-code-switching</id>
    <content type="html"><![CDATA[<p>Code-switching behavior is a common phenomenon on social media to express
solidarity or establish authority. While past work on automatic code-switching
detection depends on dictionary look-up or named-entity recognition, our
recurrent neural network model that relies on only raw features outperformed
the top systems in the EMNLP&#8217;14 Code-Switching Workshop by 17% in error rate
reduction.</p>

<p><span style='color: gray'>Final project for the Deep Learning course at CMU.</span></p>

<!--more-->


<h2>Abstract</h2>

<p>Mixed language data is one of the difficult yet less explored domains of
natural language processing. Most research in fields like machine translation
or sentiment analysis assume monolingual input. However, people who are capable
of using more than one language often communicate using multiple languages at
the same time. Sociolinguists believe this &#8220;code-switching&#8221; phenomenon to be
socially motivated. For example, to express solidarity or to establish
authority. Most past work depend on external tools or resources, such as
part-of-speech tagging, dictionary look-up, or named-entity recognizers to
extract rich features for training machine learning models. In this paper, we
train recurrent neural networks with only raw features, and use word embedding
to automatically learn meaningful representations. Using the same
mixed-language Twitter corpus, our system is able to outperform the best
SVM-based systems reported in the EMNLP&#8217;14 Code-Switching Workshop by 1% in
accuracy, or by 17% in error rate reduction.</p>

<h2>Download</h2>

<p><a class="btn btn-default" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Twitter', 'arXiv']);"  href="https://arxiv.org/abs/1412.4314" role="button">arXiv</a>
<a class="btn btn-default" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Twitter', 'PDF']);"  href="https://arxiv.org/pdf/1412.4314v2.pdf" role="button">arXiv hosted PDF</a></p>

<h2>Citation</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Chang, Joseph Chee, and Chu-Cheng Lin.
</span><span class='line'>"Recurrent-Neural-Network for Language Detection on Twitter Code-Switching Corpus."
</span><span class='line'>arXiv preprint arXiv:1412.4314 (2014).</span></code></pre></td></tr></table></div></figure>


<h2>Bibtex</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>@article{chang2014recurrent,
</span><span class='line'>  title={Recurrent-Neural-Network for Language Detection on Twitter Code-Switching Corpus},
</span><span class='line'>  author={Chang, Joseph Chee and Lin, Chu-Cheng},
</span><span class='line'>  journal={arXiv preprint arXiv:1412.4314},
</span><span class='line'>  year={2014}
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Hangman Game Solver based on Language Models]]></title>
    <link href="http://josephcc.github.com/hangman/"/>
    <updated>2012-11-04T21:14:00-08:00</updated>
    <id>http://josephcc.github.com/hangman</id>
    <content type="html"><![CDATA[<p>I&#8217;ve recently wrote a program for solving sentence-based hangman game automatically. Here&#8217;s a post
explaining my approach using the British National Corpus, <a href="http://www.speech.sri.com/projects/srilm/">srilm</a>
language model and hidden Markov model.</p>

<h2>The Hangman Game</h2>

<p>Last week, my friend <a href="http://itszero.github.com">Zero Cho</a> was asked to write a program that solves the
game of hangman automatically. The rules are
simple: the questions are short sentences or phrases with their characters
concealed, i.e. replaced with an underscore, while spaces and punctuations are revealed. The solving
process is an interactive process in which the solver program will guess a letter, and the question
system will reveal its locations. If you guessed 3 letters that are not in the sentence,
you fail the question.</p>

<p>For example, the initial hint for the answer <code>it's a sunny day</code> is <code>__'_ _ _____ ___</code>,
if a solver program guesses <code>y</code>, the system will respond with <code>__'_ _ ____y __y</code>.</p>

<!--more-->


<p>My friend Zero used a statistical approach that first guesses vows with highest frequencies until the first correct
guess. With one vow revealed, he then repeatedly choose the word with the most portion revealed to
continue guessing. His approach to guessing a half-revealed word is also statistical, namely to find the
most frequent English word that matches the pattern.  His method is explained
in detail in his recent <a href="http://itszero.github.com/blog/2012/10/29/my-way-to-write-a-hangman-ai/">blog post</a>.</p>

<p>The shortcoming of this approach is that it lacks consideration for correlations between words in the
sentences. For example, even the word <code>sorry</code> has a higher frequency than <code>sunny</code>, the latter is more
likely to appear before the word <code>day</code>. In my attempt, I trained a
<a href="http://en.wikipedia.org/wiki/Language_model">Language Model</a> to capture such correlations, and use
<a href="http://en.wikipedia.org/wiki/Hidden_Markov_model">hidden Markov model</a> to guess every word in the sentence
simultaneously.</p>

<h2>Language Model</h2>

<p>I use the state-of-the-art <a href="http://www.speech.sri.com/projects/srilm/">srilm</a> package to train my
language model on a portion of the British National Corpus (BNC). For those who are familiar with srilm,
the parameters I&#8217;ve used include order-5, Kneser-Ney discount, and interpolation. I use SWIG to
integrate srilm (in C) with my main program written in Python.</p>

<p>A language model is a statistical model that defines the probability of a sequence of words using
probability distribution trained on a free text corpus.</p>

<p>For example, given a sequence of words $w_1, w_2, w_3, &#8230;, w_n$, the unigram (one-word) language model
calculates the probability of such sequence as
$$P(w<em>{1}w</em>{2}w<em>{3}&#8230;w</em>{n}) := P(w_1) * P(w_2) * P(w_3) * &#8230; * P(w_n)$$
this is sometimes called an order-1 language model. The word probability can be calculated by counting
its appearances in the corpus:
$$ P(w) := \frac { count(w) } { |\ corpus\ | } $$
An order-2 language model, or bigram language model, defines the sequence probability using conditional
probability:
$$P(w<em>{1}w</em>{2}w<em>{3}&#8230;w</em>{n}) := P(w_1) * P(w_2|w_1) * P(w_3|w_2) * &#8230; * P(w_n|w<em>{n-1})$$
intuitively, conditional probability can be calculated as:
$$ P(w_a|w_b) := \frac { count(w</em>{b}w_{a}) } { count(w_b) } $$
Generally, higher order produces better results but also suffers more severely on the out-of-vocabulary problem
and therefore sometimes needs to fall back to lower order conditional probabilities. Many theories have been
proposed to improve the performance of language models, including absolute discount estimation, good turing,
Kneser-Ney smoothing. Formal definition of language model can be found on
<a href="http://en.wikipedia.org/wiki/Language_model">Wikipedia</a>.</p>

<h2>Hidden Markov Model</h2>

<p>Hidden Markov model is a statistical model that solves a sequence of hidden states given a sequence of
observations, state transition probability distribution, and emission probability distribution. For
example, in speech recognition, the recorded audio speech is the observation, the actual text are the
hidden states. Another example is the optical character recognition (OCR) where the scanned images are
the observations. Emission probability distribution indicates the probability from state to observation,
i.e., from text to audio or image. Language model is commonly used for state transition probability
distribution.</p>

<p>In the case solving the Hangman game, the hidden states are the English words in the answer sentences, e.g., <code>it's</code>, <code>a</code>,
<code>sunny</code>, <code>day</code>, and the observations are some half-revealed sequences, e.g., <code>__'s</code>, <code>_</code>, <code>s_nny</code>, <code>__y</code>.
The state transition probability is calculated by the trained language model, e.g., $P(day | sunny)$ in
a order-1 language model. I define the emission probability as the normalize probability of all the
English words that matches the half-revealed pattern, e.g., normalized $P(day), P(pay), P(bay), P(hey),
P(may), &#8230;, etc$. for state <code>__y</code>, and zero otherwise. With the transition and emission probabilities defined, we
can use the Viterbi dynamic programming algorithm to calculate the optimal state sequence.</p>

<p>Formal definition and more information on HMM can be found on
Wikipedia pages <a href="http://en.wikipedia.org/wiki/Hidden_Markov_model">here</a> and
<a href="http://en.wikipedia.org/wiki/Viterbi_algorithm">here</a>, or in a slide I made when I was TA for a NLP course
<a href="http://hadoop.nlpweb.org/~bizkit/lab3/vtb_example/">here</a> (in Chinese).</p>

<h2>Algorithm</h2>

<p>It is obvious that to run the hidden Markov model on a fully concealed sequence will yield bad results.
Therefore, at stage one my solver also start by guessing high frequency letters. Instead of
guessing vows only, I also use consonants, hoping that revealing some consonants will help the second
stage. The sorted list begins with <code>e, a, i, s, r, n, t, o</code>, &#8230;, etc. This
process stops until it had made one wrong guesses or have revealed more than four letters.</p>

<p>In the second stage, the trained hidden Markov model will tag the half-revealed words with English word
sequence that yield the highest language model probability and word frequencies. The solver will then
guess the letter that appears in most words. This process repeats until all characters are
revealed, and the number of failed guesses are recorded.</p>

<h2>Results</h2>

<p>Here are three examples and results. The interactive guessing progress and some internal states are
shown in the three tables below.</p>

<h3>&#8220;describe what is needed&#8221;</h3>

<p>stage   |errors |hint   | HMM prediction| guess
&#8212;&#8211;   |&#8212;&#8212;-    |&#8212;-   |&#8212;&#8212;-    | &#8212;&#8211;
1   |0  |<code>________ ____ __ ______</code>  | N/A   | e
1   |0  |<code>_e_____e ____ __ _ee_e_</code>  | N/A   | a
1   |0  |<code>_e_____e __a_ __ _ee_e_</code>  | N/A   | i
1   |0  |<code>_e___i_e __a_ i_ _ee_e_</code>  | N/A   | s
1   |0  |<code>_es__i_e __a_ is _ee_e_</code>  | N/A   | r
2   |0  |<code>_es_ri_e __a_ is _ee_e_</code>  | <strong>describe what is needed</strong>   | d
2   |0  |<code>des_ri_e __a_ is _eeded</code>  | <strong>describe what is needed</strong>   | n
2   |0  |<code>des_ri_e __a_ is needed</code>  | <strong>describe what is needed</strong>   | t
2   |0  |<code>des_ri_e __at is needed</code>  | <strong>describe what is needed</strong>   | t
2   |0  |<code>descri_e __at is needed</code>  | <strong>describe what is needed</strong>   | hbw (abrv.)
2   |0  |<code>describe what is needed</code>  | (solved)  |
<br/></p>

<h3>&#8220;an honor roll of online options&#8221;</h3>

<table>
<thead>
<tr>
<th>stage   </th>
<th>errors </th>
<th>hint   </th>
<th> HMM prediction</th>
<th> guesses</th>
</tr>
</thead>
<tbody>
<tr>
<td>1   </td>
<td>0  </td>
<td><code>__ _____ ____ __ ______ _______</code>  </td>
<td>N/A    </td>
<td> eaisr</td>
</tr>
<tr>
<td>2   </td>
<td>0  </td>
<td><code>a_ ____r r___ __ ___i_e ___i__s</code>  </td>
<td>at honor roll of police options    </td>
<td> o</td>
</tr>
<tr>
<td>2   </td>
<td>0  </td>
<td><code>a_ _o_or ro__ o_ o__i_e o__io_s</code>  </td>
<td>at honor roll of office options    </td>
<td> n</td>
</tr>
<tr>
<td>2   </td>
<td>0  </td>
<td><code>an _onor ro__ o_ on_ine o__ions</code>  </td>
<td><strong>an honor roll of online options</strong>    </td>
<td> l</td>
</tr>
<tr>
<td>2   </td>
<td>0  </td>
<td><code>an _onor roll o_ online o__ions</code>  </td>
<td><strong>an honor roll of online options</strong>    </td>
<td> t</td>
</tr>
<tr>
<td>2   </td>
<td>0  </td>
<td><code>an _onor roll o_ online o_tions</code>  </td>
<td><strong>an honor roll of online options</strong>    </td>
<td> hpf</td>
</tr>
<tr>
<td>2   </td>
<td>0  </td>
<td><code>an honor roll of online options</code>  </td>
<td>(solved)   </td>
<td></td>
</tr>
</tbody>
</table>


<p>This one shows HMM produces different results as more characters are revealed.
<br/></p>

<h3>&#8220;members of the supreme court&#8221;</h3>

<table>
<thead>
<tr>
<th>stage   </th>
<th>errors </th>
<th>hint   </th>
<th> HMM prediction</th>
<th> guesses</th>
</tr>
</thead>
<tbody>
<tr>
<td>1   </td>
<td>0  </td>
<td><code>_______ __ ___ _______ _____</code> </td>
<td> N/A   </td>
<td> e</td>
</tr>
<tr>
<td>1   </td>
<td>0  </td>
<td><code>_e__e__ __ __e ____e_e _____</code> </td>
<td> N/A   </td>
<td> a</td>
</tr>
<tr>
<td>2   </td>
<td>1  </td>
<td><code>_e__e__ __ __e ____e_e _____</code> </td>
<td> <strong>members of the supreme court</strong>  </td>
<td> rstoumchpbf</td>
</tr>
<tr>
<td>2   </td>
<td>1  </td>
<td><code>members of the supreme court</code> </td>
<td>   </td>
<td></td>
</tr>
</tbody>
</table>


<p>This last one really shows the power of language model. With only <code>e</code> revealed, the HMM process was able to guess the correct result.</p>

<h2>Improvements</h2>

<p>For stage one, instead of using just the highest frequency letters, conditional probability can also be
a factor for choosing the next letter. For example, if <code>s</code> is revealed as the second to last character
in some words, <code>t</code> and <code>e</code> may likely to be the last letter of such words.</p>

<p>For the two stages, we can decide to advance from stage 1 to 2 base on different factors instead of
heuristically. These may include the confident of the HMM results, the percentage of letters revealed,
the before mentioned conditional probabilities and more. Global optimization approaches can be used to
find good parameters to determine the timing to start stage two.  Alternatively, we can treat the two
stages as different strategies and use genetic algorithm to apply them alternately.</p>
]]></content>
  </entry>
  
</feed>
