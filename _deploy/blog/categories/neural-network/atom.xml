<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Neural Network | Joseph Chee Chang]]></title>
  <link href="http://josephcc.github.com/blog/categories/neural-network/atom.xml" rel="self"/>
  <link href="http://josephcc.github.com/"/>
  <updated>2021-07-29T10:21:38-04:00</updated>
  <id>http://josephcc.github.com/</id>
  <author>
    <name><![CDATA[Joseph Chee Chang]]></name>
    <email><![CDATA[josephcc@cmu.edu]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Twitter Code-Switching]]></title>
    <link href="http://josephcc.github.com/arXiv-twitter-code-switching/"/>
    <updated>2014-12-22T08:00:00-05:00</updated>
    <id>http://josephcc.github.com/arXiv-twitter-code-switching</id>
    <content type="html"><![CDATA[<p>Code-switching behavior is a common phenomenon on social media to express
solidarity or establish authority. While past work on automatic code-switching
detection depends on dictionary look-up or named-entity recognition, our
recurrent neural network model that relies on only raw features outperformed
the top systems in the EMNLP'14 Code-Switching Workshop by 17% in error rate
reduction.</p>

<p><span style='color: gray'>Final project for the Deep Learning course at CMU.</span></p>

<!--more-->


<h2>Abstract</h2>

<p>Mixed language data is one of the difficult yet less explored domains of
natural language processing. Most research in fields like machine translation
or sentiment analysis assume monolingual input. However, people who are capable
of using more than one language often communicate using multiple languages at
the same time. Sociolinguists believe this "code-switching" phenomenon to be
socially motivated. For example, to express solidarity or to establish
authority. Most past work depend on external tools or resources, such as
part-of-speech tagging, dictionary look-up, or named-entity recognizers to
extract rich features for training machine learning models. In this paper, we
train recurrent neural networks with only raw features, and use word embedding
to automatically learn meaningful representations. Using the same
mixed-language Twitter corpus, our system is able to outperform the best
SVM-based systems reported in the EMNLP'14 Code-Switching Workshop by 1% in
accuracy, or by 17% in error rate reduction.</p>

<h2>Download</h2>

<p><a class="btn btn-default" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Twitter', 'arXiv']);"  href="https://arxiv.org/abs/1412.4314" role="button">arXiv</a>
<a class="btn btn-default" target='_blank' onclick="_gaq.push(['_trackEvent', 'Paper', 'Twitter', 'PDF']);"  href="https://arxiv.org/pdf/1412.4314v2.pdf" role="button">arXiv hosted PDF</a></p>

<h2>Citation</h2>

<pre><code>Chang, Joseph Chee, and Chu-Cheng Lin.
"Recurrent-Neural-Network for Language Detection on Twitter Code-Switching Corpus."
arXiv preprint arXiv:1412.4314 (2014).
</code></pre>

<h2>Bibtex</h2>

<pre><code>@article{chang2014recurrent,
  title={Recurrent-Neural-Network for Language Detection on Twitter Code-Switching Corpus},
  author={Chang, Joseph Chee and Lin, Chu-Cheng},
  journal={arXiv preprint arXiv:1412.4314},
  year={2014}
}
</code></pre>
]]></content>
  </entry>
  
</feed>
